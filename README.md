# ğŸ§  Med-VLM-Bench: A Curated Benchmark Repository for Medical Vision-Language Models

ğŸ“š A comprehensive summary of recent benchmarks for evaluating and training Medical Vision-Language Models (Med-VLMs)

---

## ğŸ‘¨â€ğŸ’» Contributors

- ğŸ§‘â€ğŸ”¬ **Zanting Ye**  
  Southern Medical University  
  ğŸ“§ yzt2861252880@gmail.com

- ğŸ§‘â€ğŸ”¬ **Xu Han**  
  Shanghai Jiao Tong University  
  ğŸ“§ hanxv8826@gmail.com

- ğŸ§‘â€ğŸ”¬ **Xiaolong Niu**  
  Southern Medical University

- ğŸ§‘â€ğŸ”¬ **Zian Wang**  
  Shanghai Jiao Tong University
  
- ğŸ§‘â€ğŸ”¬ **Shengyuan Liu**  
  The Chinese University of Hong Kong   
  ğŸ“§ liushengyuan@link.cuhk.edu.hk
- ğŸ‘¨â€ğŸ« **Lijun Lu**  
  Southern Medical University
  


---

## ğŸ“Š GitHub Stats

![Stars](https://img.shields.io/github/stars/yezanting/Med-VLM-Bench-Summary?style=social)
![Forks](https://img.shields.io/github/forks/yezanting/Med-VLM-Bench-Summary?style=social)
![License](https://img.shields.io/github/license/yezanting/Med-VLM-Bench-Summary)
![Last Commit](https://img.shields.io/github/last-commit/yezanting/Med-VLM-Bench-Summary)

---

## ğŸ” Project Overview

With the continuous advancement of research on Medical Vision-Language Models (Med-VLMs) and their reasoning capabilities, a number of high-quality, publicly available datasets focusing on medical reasoning have been released between March and May 2025. These datasets provide a solid foundation for the development of multimodal medical AI systems.

**Med-VLM-Bench** is a curated, continuously updated repository of the latest and most important datasets for training and evaluating medical LLMs and VLMs. This project focuses on:

- âœ… Reasoning-centric multimodal benchmarks
- ğŸ“… Latest datasets published in Marâ€“May 2025
- ğŸ§  Foundational datasets from 2023â€“2024
- ğŸ”— Direct access to dataset links or HuggingFace/GitHub repositories

ğŸ’¡ Our knowledge is limited to public sources. We welcome community contributions â€” feel free to open an issue to share new datasets, and we will update promptly. 

ğŸ“ŒNote: The annotation time of the dataset is based on the publication time of the corresponding article.

---

## ğŸ“Š Dataset Summary Table

| Dataset Name       | Paper Title                                                                                     | Year / Venue            | Data Modality              | Task Type                      | Size                          | Download Link                                                                                      |
|--------------------|--------------------------------------------------------------------------------------------------|--------------------------|----------------------------|--------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------|
| ReasonMed| [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513) | 2025.06.11 | Text (Multi-agent CoT, Summary, QA) | Medical Reasoning, QA, CoT Fine-tuning | 370K high-quality samples distilled from 1.75M CoT paths, based on 195K questions from 4 benchmarks | [HF](https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed) |
| Lingshu (Train) | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | Text + Image (Multimodal Instruction, VQA, Report) | Multimodal Medical QA, Reasoning, Consultation, Report Generation | ~9.3M training samples from 60+ datasets | [Project Page](https://alibaba-damo-academy.github.io/lingshu/) |
| MedEvalKit (Linshu Test) | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | Text + Image (Multimodal Benchmarks) | Benchmarking: VQA, Report Generation, Medical Text QA | 152,066 evaluation samples from 16 benchmarks | [Github](https://github.com/alibaba-damo-academy/MedEvalKit) |
| MIRIAD | [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091) | 2025.06.09 | Text (Instruction-Response) | Medical QA, Retrieval-Augmented Generation (RAG), Hallucination Detection | 5.8M / 4.4M QA pairs | [HF](https://huggingface.co/datasets/miriad/miriad-5.8M) |
| ClinBench-HPB | [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095) | 2025.06.04 | Text (Multiple-choice Questions, Clinical Cases) | Medical Question Answering, Hepato-Pancreato-Biliary Clinical Case Diagnosis | 3,535 MCQs & 337 clinical cases, covering 465+ Hepato-Pancreato-Biliary diseases | [Project Page](https://clinbench-hpb.github.io), [HF](https://huggingface.co/datasets/ASD9987/ClinBench-HPB) |
| SurgVLM-DB         |SurgVLM-DB: A Large-scale Multimodal Surgical Database Comprising Over 1.81 Million Frames with 7.79 Million Conversations                                                        | 2025.06                 | Video + Text               | Multimodal QA                  | 1.81M frames, 7.79M QAs       | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                   |
| EndoBench    | [EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis](https://arxiv.org/abs/2505.23601v1) | 2025.05.29   | Image + Text (Visual QA, Multimodal Tasks, Multi-level Visual Prompts) | Endoscopy Analysis, Medical Imaging, Multimodal Model Evaluation | Covers 4 endoscopy scenarios (Gastroscopy, Colonoscopy, Capsule Endoscopy, Surgical Endoscopy); includes 12 clinical tasks and 12 subtasks; 5 levels of visual prompt granularity; 6832 clinically validated VQA samples | [HF](https://huggingface.co/datasets/Saint-lsy/EndoBench) |
| MedXpertQA | MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding | ICML2025 | Text + Image (Multimodal MCQs) | Expert-level Medical QA, Clinical Reasoning, Multimodal Understanding | 4,460 questions (2,455 text / 2,005 image) | [HF](https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA) |
| MedCaseReasoning   | MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports                         | 2025.05.20              | Text                       | Diagnostic Reasoning           | 14,489 QA cases               | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                   |
| vlm-project-with-images-with-bbox-images-with-tree-of-thoughts | - | 2025.06                 | MRI Image + BBox + Multilingual QA (Including 7 languages: vi, en, fr, de, zh, ko, ja) | VQA, Lesion Detection, Clinical Reasoning (Tree-of-Thought) | 12.3k samples | [HF](https://huggingface.co/datasets/tungvu3196/vlm-project-with-images-with-bbox-images-with-tree-of-thoughts) | N/A |
| DrVD-Bench         | DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?                | 2025.05.30              | Medical Images + Text      | VQA, Reasoning, Report Gen.    | 7,789 imageâ€“QA pairs          | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine                                     | 2025.05                 | Text                       | Instruction Tuning             | 5M instances, 19K instructions| [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                            |
| MedS-Bench         | -                                                                                                | 2025.05                 | Text                       | Clinical Task Benchmark        | 11 task types                 | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                          |
| MM-Skin            | MM-Skin:Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks                    | 2025.05.09              | Image + Text               | Open-ended VQA (no reasoning)  | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                        |
| AlphaMed19K        | Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL                                                     | 2025.05.23              | Text                       | QA Reasoning                   | 19K QAs                       | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                           |
| Surg-396K | [EndoChat: Grounded Multimodal Large Language Model for Endoscopic Surgery](https://arxiv.org/abs/2501.11347) | 2025.03.15 | Image + Text (Multimodal Instruction, VQA, Grounding, Description) | Endoscopic Surgery, Surgical Scene Understanding, Visual QA, Grounded Dialogue | 396K instruction-image pairs from 41.4K images across 3 datasets (EndoVis, CoPESD, Cholec80) with 5 conversation types and 7 scene understanding tasks | [GitHub](https://github.com/gkw0010/EndoChat), [Data Link](https://mycuhk-my.sharepoint.com/personal/1155180074_link_cuhk_edu_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1155180074%5Flink%5Fcuhk%5Fedu%5Fhk%2FDocuments%2FRENLAB%20AI%20Data%2FSurg%2D396k&ga=1) |
| HuatuoGPT-o1 Dataset | [HuatuoGPT-o1ï¼ŒTowards Medical Complex Reasoning with LLMs](https://arxiv.org/pdf/2412.18925) | 2024.12.25 | Text (Complex CoT, Medical Verifiable Problems, Multi-Step Reasoning) | Medical Complex Reasoning, CoT Fine-tuning, Reinforcement Learning | Contains 40K high-quality medical complex reasoning problems filtered by a medical verifier, based on MedQA-USMLE and MedMCQA medical exam training sets | [GitHub](https://github.com/FreedomIntelligence/HuatuoGPT-o1), [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) |
| PubMedVision        | [HuatuoGPT-Visionn, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale](https://arxiv.org/abs/2406.19280)                         | 2024.09.30 | Image + Text (Multimodal) | Medical VQA (Alignment VQA, Instruction-Tuning VQA), Captioning, Summarization | 1.3M VQA samples from 914,960 filtered PubMed medical images & text (647K + 647K)     | [Hugging Face](https://huggingface.co/datasets/FreedomIntelligence/PubMedVision)|
| PMC-VQA            | PMC-VQA: Visual Instruction Tuning for Medical VQA                                                       | 2024.08.08              | Image + Text               | VQA                            | 226,946 QA pairs              | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                            |
| VQARad             | - | 2023.08.07              | Radiography                | VQA                            | 315 images, 3,515 QAs         | [OSF](https://osf.io/89kps/)                                                                         |
|  Asclepius | [Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2402.11217) | ACL 2025 | Image + Text | VQA | 3232 VQA pairs encompassing 15 medical specialties, stratifying into 3 main categories and 8 sub-categories of clinical tasks | [GitHub](https://github.com/Asclepius-Med/Asclepius) |
|  MedTrinity-25M | [MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine](https://yunfeixie233.github.io/MedTrinity-25M/) | ICLR 2025 | Image + Text | VQA | 25M VQA pairs, covering over 25 million images across 10 modalities | [HF](https://huggingface.co/datasets/UCSC-VLAA/MedTrinity-25M) |
|  MediConfusion | [MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models](https://arxiv.org/abs/2409.15477) | ICLR 2025 | Image + Text | VQA | 176 confusing pairs, a set of two images that share the same question and corresponding answer options, but the correct answer is different for the images. | [HF](https://huggingface.co/datasets/shahab7899/MediConfusion) |
| GMAI-MMBench       | GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI                     | NeurIPS 2024            | Multi-modal (38 types)     | VQA                            | 26K QA pairs                  | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                       |
| PathMMU            | PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology       | 2024.03.20              | Pathology Image + Text     | Multi-choice, Reasoning        | 33,428 QAs, 24,067 images     | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                             |
| OmniMedVQA         | OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM                          | CVPR 2024               | Multi-modal (12 types)     | VQA                            | 118,010 images, 127,995 QA    | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                       |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models                 | NeurIPS 2024            | Medical Images + QA        | Open/Closed QA                 | 41K QA pairs                  | [GitHub](https://github.com/richard-peng-xia/CARES)                                                |
| MultiMedEval       | MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models                        | 2024.02.16              | Image + Text               | Multi-task Evaluation          | 6 tasks, 23 datasets          | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                             |

---

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆæœ¬

### ğŸ” é¡¹ç›®ç®€ä»‹

éšç€åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-VLMï¼‰åŠå…¶æ¨ç†èƒ½åŠ›ç ”ç©¶çš„æŒç»­æ¨è¿›ï¼Œå°¤å…¶åœ¨ 2025 å¹´ 3 æœˆè‡³ 5 æœˆæœŸé—´ï¼Œé™†ç»­å‘å¸ƒäº†ä¼—å¤šé«˜è´¨é‡ã€èšç„¦äºåŒ»å­¦æ¨ç†èƒ½åŠ›çš„æ–°å‹å…¬å¼€æ•°æ®é›†ï¼Œä¸ºå¤šæ¨¡æ€åŒ»ç–—äººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½æ±‡æ€»è¿™ä¸€äº›æ•°æ®ï¼ŒæœŸå¾…èƒ½ä¸ºè¯¥ç¤¾åŒºæä¾›æ›´ä¾¿æ·çš„æ•°æ®è®¿é—®æ–¹å¼ã€‚æˆ‘ä»¬å‘å¸ƒäº†Med-VLM-Benchï¼š

**Med-VLM-Bench** è‡´åŠ›äºæ±‡æ€»å¹¶æ•´ç†è¿™äº›æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°çš„å…³é”®èµ„æºï¼š

- âœ… èšç„¦ 2025 å¹´ 3â€“6 æœˆå‘å¸ƒçš„æ–°æ•°æ®é›†  
- ğŸ§  é‡ç‚¹å¼ºè°ƒæ¨ç†èƒ½åŠ›ã€å¤šæ¨¡æ€ç†è§£å’Œé—®ç­”èƒ½åŠ›çš„æ•°æ®é›† 
- ğŸ§ª åŒæ—¶è¦†ç›– 2023â€“2024 å¹´çš„ç»å…¸Med LLM/VLM benchmark datasets
- ğŸ”— æä¾›ç›´æ¥å¯ç”¨çš„ä¸‹è½½é“¾æ¥å’Œå¼€æºåœ°å€

ğŸ’¡ æˆ‘ä»¬çš„çŸ¥è¯†æ¥æºæœ‰é™ï¼Œæ¬¢è¿å¤§å®¶é€šè¿‡ Issue æˆ– PR æ¨èæ›´å¤šæ•°æ®é›†ï¼Œæˆ‘ä»¬ä¼šç¬¬ä¸€æ—¶é—´æ›´æ–°ï¼

ğŸ“ŒNote: æ­¤å¤–æˆ‘ä»¬çš„æ•°æ®é›†æ ‡æ³¨æ—¶é—´ä»¥ç›¸åº”æ–‡ç« å‘è¡¨æ—¶é—´ä¸ºå‡†

---

### ğŸ“Š æ•°æ®é›†æ±‡æ€»è¡¨

| æ•°æ®é›†åç§°         | è®ºæ–‡æ ‡é¢˜                                                                 | å¹´ä»½ / ä¼šè®®              | æ•°æ®æ¨¡æ€                  | ä»»åŠ¡ç±»å‹                        | æ•°æ®è§„æ¨¡                       | ä¸‹è½½é“¾æ¥                                                                                         |
|--------------------|--------------------------------------------------------------------------|---------------------------|---------------------------|----------------------------------|-------------------------------|--------------------------------------------------------------------------------------------------|
| ReasonMed | [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513) | 2025.06.11 | æ–‡æœ¬ï¼ˆå¤šä»£ç†æ¨ç†ã€å¤šæ­¥æ€»ç»“ã€åŒ»å­¦é—®ç­”ï¼‰ | åŒ»å­¦æ¨ç†ã€é—®ç­”ã€é“¾å¼æ€ç»´å¾®è°ƒ | ä» 175 ä¸‡æ¡ CoT è·¯å¾„ä¸­ç²¾ç‚¼å‡ºçš„ 37 ä¸‡é«˜è´¨é‡æ ·æœ¬ï¼Œè¦†ç›–æ¥è‡ª 4 ä¸ªåŸºå‡†çš„ 19.5 ä¸‡é—®é¢˜ | [HF](https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed) |
| Lingshuï¼ˆTrainï¼‰ | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | æ–‡æœ¬ + å›¾åƒï¼ˆå¤šæ¨¡æ€æŒ‡ä»¤ã€VQAã€æŠ¥å‘Šï¼‰ | å¤šæ¨¡æ€åŒ»å­¦é—®ç­”ã€æ¨ç†ã€é—®è¯Šã€æŠ¥å‘Šç”Ÿæˆ | çº¦ 930 ä¸‡è®­ç»ƒæ ·æœ¬ï¼Œæ¥è‡ª 60+ æ•°æ®é›† | [Project Page](https://alibaba-damo-academy.github.io/lingshu/) |
| MedEvalKitï¼ˆLinshu testï¼‰ | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | æ–‡æœ¬ + å›¾åƒï¼ˆå¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ï¼‰ | åŸºå‡†è¯„æµ‹ï¼šVQAã€æŠ¥å‘Šç”Ÿæˆã€åŒ»å­¦æ–‡æœ¬é—®ç­” | å…± 152,066 ä¸ªè¯„ä¼°æ ·æœ¬ï¼Œæ¥è‡ª 16 ä¸ªåŸºå‡†æ•°æ®é›† | [Github](https://github.com/alibaba-damo-academy/MedEvalKit) |
| MIRIAD | [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091) | 2025.06.09 | æ–‡æœ¬ï¼ˆæŒ‡ä»¤-å›ç­”å¯¹ï¼‰ | åŒ»å­¦é—®ç­”ã€RAG æ£€ç´¢å¢å¼ºã€å¹»è§‰æ£€æµ‹ |  582 ä¸‡ / 448 ä¸‡ | [HF](https://huggingface.co/datasets/miriad/miriad-5.8M) |
| ClinBench-HPB | [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095) | 2025.06.04 | æ–‡æœ¬ (é€‰æ‹©é¢˜, ä¸´åºŠç—…ä¾‹) | åŒ»å­¦é—®ç­”, ä¸´åºŠç—…ä¾‹è¯Šæ–­ | 3,535é“é€‰æ‹©é¢˜å’Œ337ä¸ªä¸´åºŠç—…ä¾‹, è¦†ç›–465+ç§è‚èƒ†èƒ°ç–¾ç—… | [Project Page](https://clinbench-hpb.github.io), [HF](https://huggingface.co/datasets/ASD9987/ClinBench-HPB) |
| SurgVLM-DB         |SurgVLM-DB: A Large-Scale Multimodal Surgical Database                               | 2025.06                   | è§†é¢‘ + æ–‡æœ¬               | å¤šæ¨¡æ€é—®ç­”                      | 1.81Må¸§, 7.79Må¯¹è¯             | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                 |
| EndoBench  | [EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis](https://arxiv.org/abs/2505.23601v1) | 2025.05.29 | å›¾åƒ+æ–‡æœ¬ï¼ˆè§†è§‰é—®ç­”ã€å¤šæ¨¡æ€ä»»åŠ¡ã€å¤šå±‚æ¬¡è§†è§‰æç¤ºï¼‰ | å†…é•œåˆ†æã€åŒ»å­¦å½±åƒã€å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼° | è¦†ç›–èƒƒé•œã€ç»“è‚ é•œã€èƒ¶å›Šå†…é•œå’Œæ‰‹æœ¯å†…é•œ 4 å¤§åœºæ™¯ï¼›åŒ…å« 12 ä¸ªä¸´åºŠä»»åŠ¡åŠ 12 ä¸ªæ¬¡ä»»åŠ¡ï¼›5 ç§è§†è§‰æç¤ºç²’åº¦ï¼›6832 ä¸ªç»è¿‡ä¸´åºŠéªŒè¯çš„ VQA æ ·æœ¬ | [HF](https://huggingface.co/datasets/Saint-lsy/EndoBench) |
| MedXpertQA | MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding | ICML2025 | æ–‡æœ¬ + å›¾åƒï¼ˆå¤šæ¨¡æ€é€‰æ‹©é¢˜ï¼‰ | ä¸“å®¶çº§åŒ»å­¦é—®ç­”ã€ä¸´åºŠæ¨ç†ã€å¤šæ¨¡æ€ç†è§£ | å…± 4,460 é¢˜ï¼ˆæ–‡æœ¬ 2,455 / å›¾åƒ 2,005ï¼‰ | [HF](https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA) |
| MedCaseReasoning   |MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports  | 2025.05.20                | æ–‡æœ¬                      | è¯Šæ–­æ¨ç†                        | 14,489é—®ç­”å¯¹                   | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                 |
| vlm-project-with-images-with-bbox-images-with-tree-of-thoughts | - | 2025.06 | MRI å›¾åƒ + BBox + å¤šè¯­ç§é—®ç­”ï¼ˆvi, en, fr, de, zh, ko, jaï¼‰ | åŒ»å­¦VQAã€ç—…ç¶æ£€æµ‹ã€ä¸´åºŠæ¨ç†ï¼ˆTree-of-Thoughtï¼‰ | 12,325 æ¡æ ·æœ¬ | [HF](https://huggingface.co/datasets/tungvu3196/vlm-project-with-images-with-bbox-images-with-tree-of-thoughts) | æœªæ³¨æ˜ |
| DrVD-Bench         | DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis? | 2025.05.30          | åŒ»å­¦å›¾åƒ + æ–‡æœ¬           | åŒ»å­¦VQAã€æ¨ç†ã€æŠ¥å‘Šç”Ÿæˆ         | 7,789å›¾æ–‡QAå¯¹                  | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine              | 2025.05                   | æ–‡æœ¬                      | æŒ‡ä»¤å¾®è°ƒ                        | 5Mæ ·æœ¬, 19KæŒ‡ä»¤                | [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                         |
| MedS-Bench         | -                                                                         | 2025.05                   | æ–‡æœ¬                      | ä¸´åºŠä»»åŠ¡è¯„ä¼°                    | 11å¤§ç±»ä»»åŠ¡                     | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                       |
| MM-Skin            | MM-Skin:Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks | 2025.05.09          | å›¾åƒ + æ–‡æœ¬               | å¼€æ”¾å¼VQAï¼ˆæ— æ¨ç†ï¼‰             | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                      |
| AlphaMed19K        | Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL                              | 2025.05.23                | æ–‡æœ¬                      | æ¨ç†é—®ç­”                        | 19Ké—®ç­”å¯¹                      | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                         |
| Surg-396K | [EndoChat: Grounded Multimodal Large Language Model for Endoscopic Surgery](https://arxiv.org/abs/2501.11347) | 2025.03.15 | å›¾åƒ + æ–‡æœ¬ï¼ˆå¤šæ¨¡æ€æŒ‡ä»¤ã€é—®ç­”ã€ç›®æ ‡å®šä½ã€è¯¦ç»†æè¿°ï¼‰ | å†…çª¥é•œå¤–ç§‘ã€æ‰‹æœ¯åœºæ™¯ç†è§£ã€è§†è§‰é—®ç­”ã€å®šä½å¯¹è¯ | æ¥è‡ª EndoVisã€CoPESD å’Œ Cholec80 çš„ 41,400 å¼ å›¾åƒï¼Œç”Ÿæˆ 396,000 å›¾æ–‡å¯¹ï¼Œè¦†ç›– 5 ç§å¯¹è¯ç±»å‹ä¸ 7 ç±»æ‰‹æœ¯ç†è§£ä»»åŠ¡ | [GitHub](https://github.com/gkw0010/EndoChat) [Data link](https://mycuhk-my.sharepoint.com/personal/1155180074_link_cuhk_edu_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1155180074%5Flink%5Fcuhk%5Fedu%5Fhk%2FDocuments%2FRENLAB%20AI%20Data%2FSurg%2D396k&ga=1)|
| HuatuoGPT-o1 Dataset | [HuatuoGPT-o1ï¼ŒTowards Medical Complex Reasoning with LLMs](https://arxiv.org/pdf/2412.18925) | 2024.12.25 | æ–‡æœ¬ï¼ˆå¤æ‚é“¾å¼æ€ç»´ã€åŒ»å­¦éªŒè¯é¢˜ã€å¤šæ­¥æ¨ç†ï¼‰ | åŒ»å­¦å¤æ‚æ¨ç†ã€é“¾å¼æ€ç»´å¾®è°ƒã€å¼ºåŒ–å­¦ä¹  | åŒ…å« 40K ç»åŒ»å­¦éªŒè¯å™¨ç­›é€‰çš„é«˜è´¨é‡åŒ»å­¦å¤æ‚æ¨ç†é—®é¢˜ï¼ŒåŸºäº MedQA-USMLE å’Œ MedMCQA åŒ»å­¦è€ƒè¯•è®­ç»ƒé›† | [GitHub](https://github.com/FreedomIntelligence/HuatuoGPT-o1), [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) |
| PubMedVision         | [HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale](https://arxiv.org/abs/2406.19280)                         | 2024.09.30 | å›¾åƒ + æ–‡æœ¬ï¼ˆå¤šæ¨¡æ€ï¼‰     | åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€å›¾æ–‡å¯¹é½ã€æŒ‡ä»¤å¾®è°ƒã€æè¿°ç”Ÿæˆç­‰                         | 130 ä¸‡ VQA æ ·æœ¬ï¼Œæ¥è‡ª PubMed ä¸­ç­›é€‰çš„ 91.5 ä¸‡åŒ»å­¦å›¾åƒä¸ä¸Šä¸‹æ–‡ï¼ˆ647K + 647Kï¼‰        | [HF](https://huggingface.co/datasets/FreedomIntelligence/PubMedVision)      |
| PMC-VQA            | PMC-VQA: Visual Instruction Tuning for Medical VQA                                 | 2024.08.08                | å›¾åƒ + æ–‡æœ¬               | åŒ»å­¦VQA                         | 226,946é—®ç­”å¯¹                  | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                          |
| VQARad             |-| 2023.08.07                | æ”¾å°„å›¾åƒ                  | VQA                             | 315å›¾åƒ, 3515é—®ç­”              | [OSF](https://osf.io/89kps/)                                                                       |
|  Asclepius | [Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2402.11217) | ACL 2025 | å›¾åƒ + æ–‡æœ¬ | åŒ»å­¦VQA | 3232æ¡é—®ç­”å¯¹ï¼Œæ¶µç›– 15 ä¸ªåŒ»å­¦ä¸“ä¸šï¼Œåˆ†ä¸º 3 ä¸ªä¸»è¦ç±»åˆ«å’Œ 8 ä¸ªå­ç±»åˆ«çš„ä¸´åºŠä»»åŠ¡ | [GitHub](https://github.com/Asclepius-Med/Asclepius) |
|  MedTrinity-25M | [MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine](https://yunfeixie233.github.io/MedTrinity-25M/) | ICLR 2025 | å›¾åƒ + æ–‡æœ¬ | åŒ»å­¦VQA | å¤§è§„æ¨¡åŒ»å­¦å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›– 10 ç§æ¨¡æ€çš„2500ä¸‡å¼ å›¾åƒï¼Œä¸º65ç§ç–¾ç—…æä¾›å¤šç²’åº¦æ³¨é‡Š | [HF](https://huggingface.co/datasets/UCSC-VLAA/MedTrinity-25M) |
|  MediConfusion | [MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models](https://arxiv.org/abs/2409.15477) | ICLR 2025 | å›¾åƒ + æ–‡æœ¬ | åŒ»å­¦VQA | ç”± 176 ä¸ªä»¤äººå›°æƒ‘çš„å¯¹ç»„æˆã€‚æ··æ·†å¯¹æ˜¯ä¸€ç»„ä¸¤å¼ å›¾åƒï¼Œå®ƒä»¬å…±äº«ç›¸åŒçš„é—®é¢˜å’Œç›¸åº”çš„ç­”æ¡ˆé€‰é¡¹ï¼Œä½†å›¾åƒçš„æ­£ç¡®ç­”æ¡ˆä¸åŒã€‚ | [HF](https://huggingface.co/datasets/shahab7899/MediConfusion) |
| GMAI-MMBench       | GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI | NeurIPS 2024          | å¤šæ¨¡æ€ï¼ˆ38ç§ï¼‰            | åŒ»å­¦VQA                         | 26Ké—®ç­”å¯¹                      | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                     |
| PathMMU            | PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Patholog | 2024.03.20      | ç—…ç†å›¾åƒ + æ–‡æœ¬           | é€‰æ‹©é¢˜+æ¨ç†                     | 33,428é—®ç­”, 24,067å›¾åƒ         | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                           |
| OmniMedVQA         | OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM    | CVPR 2024                 | å¤šæ¨¡æ€ï¼ˆ12ç§ï¼‰            | åŒ»å­¦VQA                         | 118,010å›¾åƒ, 127,995é—®ç­”       | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                     |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models | NeurIPS 2024         | åŒ»å­¦å›¾åƒ+é—®ç­”             | å¼€æ”¾ä¸å°é—­é—®ç­”                   | 41Ké—®ç­”å¯¹                      | [GitHub](https://github.com/richard-peng-xia/CARES)                                              |
| MultiMedEval       | MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models  | 2024.02.16                | å›¾æ–‡å¤šæ¨¡æ€                | å¤šä»»åŠ¡è¯„ä¼°                      | 6ä»»åŠ¡, 23æ•°æ®é›†                | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                           |

---

## ğŸ“¬ è”ç³»æˆ‘ä»¬ / Issues & Contact

å¦‚æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿æäº¤ Issue æˆ–é€šè¿‡é‚®ä»¶è”ç³»ï¼š

- ğŸ“§ **å¶èµæŒº (Zanting Ye)**: yzt2861252880@gmail.com  
- ğŸ“§ **éŸ©ç»ª (Xu Han)**: hanxv8826@gmail.com

---

## ğŸ‘¥ åˆä½œè€…ï¼ˆContributorsï¼‰

<table>
  <tr>
    <td align="center">
      <a href="https://github.com/yezanting">
        <img src="https://github.com/yezanting.png" width="80px;" alt="yezanting"/><br />
        <sub><b>å¶èµæŒº(Zanting Ye)</b></sub><br />
        <a href="mailto:yzt2861252880@gmail.com">ğŸ“§</a>
      </a>
    </td>
    <td align="center">
      <a href="https://github.com/lailainan">
        <img src="https://github.com/lailainan.png" width="80px;" alt="lailainan"/><br />
        <sub><b>éŸ©ç»ª(Xu Han)</b></sub><br />
        <a href="mailto:hanxv8826@gmail.com">ğŸ“§</a>
      </a>
    </td>
        <td align="center">
          <a href="https://github.com/Long0121">
          <img src="https://github.com/Long0121.png" width="80px;" alt="lailainan"/><br />
          <sub><b>ç‰›å°é¾™(Xiaolong Niu)</b></sub><br />
          <a href="">ğŸ“§</a>
     </td>
        <td align="center">
          <a href="https://github.com/zianwang1110">
          <img src="https://github.com/zianwang1110.png" width="80px;" alt="lailainan"/><br />
          <sub><b>ç‹æ¢“å®‰(Zian Wang)</b></sub><br />
          <a href="">ğŸ“§</a>  
      </a>
    </td>
        <td align="center">
          <a href="https://github.com/Saint-lsy">
          <img src="https://github.com/Saint-lsy.png" width="80px;" alt="lailainan"/><br />
          <sub><b>åˆ˜åœ£åœ†(Shengyuan Liu)</b></sub><br />
          <a href="liushengyuan@link.cuhk.edu.hk">ğŸ“§</a> 
      </a>
    </td>
  </tr>
</table>

---

## â­ Star è¶‹åŠ¿å›¾ (Star History)

![Star History Chart](https://api.star-history.com/svg?repos=yezanting/Med-VLM-Bench-Summary&type=Date)

---

---

â­ Star æœ¬é¡¹ç›®æ”¯æŒæˆ‘ä»¬æŒç»­æ›´æ–°ï¼Œæ¬¢è¿ PR å’Œå»ºè®®äº¤æµï¼
