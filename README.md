# Med-VLM-Bench-Summary
A Curated Benchmark Repository for Medical Vision-Language Models
# ğŸ§  Med-VLM-Bench: A Curated Benchmark Repository for Medical Vision-Language Models

ğŸ“š A comprehensive summary of recent benchmarks for evaluating and training Medical Vision-Language Models (Med-VLMs)

---

## ğŸ‘¨â€ğŸ’» Authors

| Name            | Affiliation                     | Contact                         |
|-----------------|----------------------------------|----------------------------------|
| Zanting Ye      | Southern Medical University     | yzt2861252880@gmail.com         |
| Xu Han          | Shanghai Jiao Tong University   | hanxv8826@gmail.com             |
| Xiaolong Niu    | Southern Medical University     | -                                |
| Lijun Lu        | Southern Medical University     | -                                |

---

## ğŸ“Š GitHub Stats

![Stars](https://img.shields.io/github/stars/yezanting/Med-VLM-Bench-Summary?style=social)
![Forks](https://img.shields.io/github/forks/yezanting/Med-VLM-Bench-Summary?style=social)
![License](https://img.shields.io/github/license/yezanting/Med-VLM-Bench-Summary)
![Last Commit](https://img.shields.io/github/last-commit/yezanting/Med-VLM-Bench-Summary)

---

## ğŸ” Project Overview

With the continuous advancement of research on Medical Vision-Language Models (Med-VLMs) and their reasoning capabilities, a number of high-quality, publicly available datasets focusing on medical reasoning have been released between March and May 2025. These datasets provide a solid foundation for the development of multimodal medical AI systems.

**Med-VLM-Bench** is a curated, continuously updated repository of the latest and most important datasets for training and evaluating medical LLMs and VLMs. This project focuses on:

- âœ… Reasoning-centric multimodal benchmarks
- ğŸ“… Latest datasets published in Marâ€“May 2025
- ğŸ§  Foundational datasets from 2023â€“2024
- ğŸ”— Direct access to dataset links or HuggingFace/GitHub repositories

ğŸ’¡ Our knowledge is limited to public sources. We welcome community contributions â€” feel free to open an issue to share new datasets, and we will update promptly.

---

## ğŸ“Š Dataset Summary Table

| Dataset Name       | Paper Title                                                                                     | Year / Venue            | Data Modality              | Task Type                      | Size                          | Download Link                                                                                      |
|--------------------|--------------------------------------------------------------------------------------------------|--------------------------|----------------------------|--------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------|
| SurgVLM-DB         | A Large-Scale Multimodal Surgical Database                                                      | 2025.06                 | Video + Text               | Multimodal QA                  | 1.81M frames, 7.79M QAs       | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                   |
| MedCaseReasoning   | Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports                         | 2025.05.20              | Text                       | Diagnostic Reasoning           | 14,489 QA cases               | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                   |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine                                     | 2025.05                 | Text                       | Instruction Tuning             | 5M instances, 19K instructions| [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                            |
| MedS-Bench         | -                                                                                                | 2025.05                 | Text                       | Clinical Task Benchmark        | 11 task types                 | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                          |
| DrVD-Bench         | Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?                | 2025.05.30              | Medical Images + Text      | VQA, Reasoning, Report Gen.    | 7,789 imageâ€“QA pairs          | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MM-Skin            | Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks                    | 2025.05.09              | Image + Text               | Open-ended VQA (no reasoning)  | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                        |
| AlphaMed19K        | LLM Reasoning with Minimalist Rule-Based RL                                                     | 2025.05.23              | Text                       | QA Reasoning                   | 19K QAs                       | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                           |
| PMC-VQA            | Visual Instruction Tuning for Medical VQA                                                       | 2024.08.08              | Image + Text               | VQA                            | 226,946 QA pairs              | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                            |
| VQARad             | VQA Benchmark for Radiography Images                                                            | 2023.08.07              | Radiography                | VQA                            | 315 images, 3,515 QAs         | [OSF](https://osf.io/89kps/)                                                                         |
| GMAI-MMBench       | A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI                     | NeurIPS 2024            | Multi-modal (38 types)     | VQA                            | 26K QA pairs                  | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                       |
| PathMMU            | A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology       | 2024.03.20              | Pathology Image + Text     | Multi-choice, Reasoning        | 33,428 QAs, 24,067 images     | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                             |
| OmniMedVQA         | A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM                          | CVPR 2024               | Multi-modal (12 types)     | VQA                            | 118,010 images, 127,995 QA    | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                       |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models                 | NeurIPS 2024            | Medical Images + QA        | Open/Closed QA                 | 41K QA pairs                  | [GitHub](https://github.com/richard-peng-xia/CARES)                                                |
| MultiMedEval       | A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models                        | 2024.02.16              | Image + Text               | Multi-task Evaluation          | 6 tasks, 23 datasets          | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                             |

---

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆæœ¬

### ğŸ” é¡¹ç›®ç®€ä»‹

éšç€åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-VLMï¼‰åŠå…¶æ¨ç†èƒ½åŠ›ç ”ç©¶çš„æŒç»­æ¨è¿›ï¼Œå°¤å…¶åœ¨ 2025 å¹´ 3 æœˆè‡³ 5 æœˆæœŸé—´ï¼Œé™†ç»­å‘å¸ƒäº†ä¼—å¤šé«˜è´¨é‡ã€èšç„¦äºåŒ»å­¦æ¨ç†èƒ½åŠ›çš„æ–°å‹å…¬å¼€æ•°æ®é›†ï¼Œä¸ºå¤šæ¨¡æ€åŒ»ç–—äººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½æ±‡æ€»è¿™ä¸€äº›æ•°æ®ï¼ŒæœŸå¾…èƒ½ä¸ºè¯¥ç¤¾åŒºæä¾›æ›´ä¾¿æ·çš„æ•°æ®è®¿é—®æ–¹å¼ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å‘å¸ƒäº†Med-VLM-Benchï¼š

**Med-VLM-Bench** è‡´åŠ›äºæ±‡æ€»å¹¶æ•´ç†è¿™äº›æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°çš„å…³é”®èµ„æºï¼š

- âœ… èšç„¦ 2025 å¹´ 3â€“6 æœˆå‘å¸ƒçš„æ–°æ•°æ®é›†  
- ğŸ§  é‡ç‚¹å¼ºè°ƒæ¨ç†èƒ½åŠ›ã€å¤šæ¨¡æ€ç†è§£å’Œé—®ç­”èƒ½åŠ›çš„æ•°æ®é›† 
- ğŸ§ª åŒæ—¶è¦†ç›– 2023â€“2024 å¹´çš„ç»å…¸Med LLM/VLM benchmark datasets
- ğŸ”— æä¾›ç›´æ¥å¯ç”¨çš„ä¸‹è½½é“¾æ¥å’Œå¼€æºåœ°å€

ğŸ’¡ æˆ‘ä»¬çš„çŸ¥è¯†æ¥æºæœ‰é™ï¼Œæ¬¢è¿å¤§å®¶é€šè¿‡ Issue æˆ– PR æ¨èæ›´å¤šæ•°æ®é›†ï¼Œæˆ‘ä»¬ä¼šç¬¬ä¸€æ—¶é—´æ›´æ–°ï¼

---

### ğŸ“Š æ•°æ®é›†æ±‡æ€»è¡¨

| æ•°æ®é›†åç§°         | è®ºæ–‡æ ‡é¢˜                                                                 | å¹´ä»½ / ä¼šè®®              | æ•°æ®æ¨¡æ€                  | ä»»åŠ¡ç±»å‹                        | æ•°æ®è§„æ¨¡                       | ä¸‹è½½é“¾æ¥                                                                                         |
|--------------------|--------------------------------------------------------------------------|---------------------------|---------------------------|----------------------------------|-------------------------------|--------------------------------------------------------------------------------------------------|
| SurgVLM-DB         | A Large-Scale Multimodal Surgical Database                               | 2025.06                   | è§†é¢‘ + æ–‡æœ¬               | å¤šæ¨¡æ€é—®ç­”                      | 1.81Må¸§, 7.79Må¯¹è¯             | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                 |
| MedCaseReasoning   | Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports  | 2025.05.20                | æ–‡æœ¬                      | è¯Šæ–­æ¨ç†                        | 14,489é—®ç­”å¯¹                   | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                 |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine              | 2025.05                   | æ–‡æœ¬                      | æŒ‡ä»¤å¾®è°ƒ                        | 5Mæ ·æœ¬, 19KæŒ‡ä»¤                | [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                         |
| MedS-Bench         | -                                                                         | 2025.05                   | æ–‡æœ¬                      | ä¸´åºŠä»»åŠ¡è¯„ä¼°                    | 11å¤§ç±»ä»»åŠ¡                     | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                       |
| DrVD-Bench         | Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis? | 2025.05.30          | åŒ»å­¦å›¾åƒ + æ–‡æœ¬           | åŒ»å­¦VQAã€æ¨ç†ã€æŠ¥å‘Šç”Ÿæˆ         | 7,789å›¾æ–‡QAå¯¹                  | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MM-Skin            | Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks | 2025.05.09          | å›¾åƒ + æ–‡æœ¬               | å¼€æ”¾å¼VQAï¼ˆæ— æ¨ç†ï¼‰             | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                      |
| AlphaMed19K        | LLM Reasoning with Minimalist Rule-Based RL                              | 2025.05.23                | æ–‡æœ¬                      | æ¨ç†é—®ç­”                        | 19Ké—®ç­”å¯¹                      | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                         |
| PMC-VQA            | Visual Instruction Tuning for Medical VQA                                | 2024.08.08                | å›¾åƒ + æ–‡æœ¬               | åŒ»å­¦VQA                         | 226,946é—®ç­”å¯¹                  | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                          |
| VQARad             | VQA Benchmark for Radiography Images                                     | 2023.08.07                | æ”¾å°„å›¾åƒ                  | VQA                             | 315å›¾åƒ, 3515é—®ç­”              | [OSF](https://osf.io/89kps/)                                                                       |
| GMAI-MMBench       | A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI | NeurIPS 2024          | å¤šæ¨¡æ€ï¼ˆ38ç§ï¼‰            | åŒ»å­¦VQA                         | 26Ké—®ç­”å¯¹                      | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                     |
| PathMMU            | A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology | 2024.03.20      | ç—…ç†å›¾åƒ + æ–‡æœ¬           | é€‰æ‹©é¢˜+æ¨ç†                     | 33,428é—®ç­”, 24,067å›¾åƒ         | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                           |
| OmniMedVQA         | A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM    | CVPR 2024                 | å¤šæ¨¡æ€ï¼ˆ12ç§ï¼‰            | åŒ»å­¦VQA                         | 118,010å›¾åƒ, 127,995é—®ç­”       | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                     |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models | NeurIPS 2024         | åŒ»å­¦å›¾åƒ+é—®ç­”             | å¼€æ”¾ä¸å°é—­é—®ç­”                   | 41Ké—®ç­”å¯¹                      | [GitHub](https://github.com/richard-peng-xia/CARES)                                              |
| MultiMedEval       | A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models  | 2024.02.16                | å›¾æ–‡å¤šæ¨¡æ€                | å¤šä»»åŠ¡è¯„ä¼°                      | 6ä»»åŠ¡, 23æ•°æ®é›†                | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                           |

---

## ğŸ“¬ è”ç³»æ–¹å¼

å¦‚æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿è”ç³»æˆ–æäº¤ Issueï¼š

- ğŸ“§ å¶èµæŒº (Zanting Ye): yzt2861252880@gmail.com  
- ğŸ“§ éŸ©ç»ª (Xu Han): hanxv8826@gmail.com

---

â­ Star æœ¬é¡¹ç›®æ”¯æŒæˆ‘ä»¬æŒç»­æ›´æ–°ï¼Œæ¬¢è¿ PR å’Œå»ºè®®äº¤æµï¼
