# üß† Med-VLM-Bench: A Curated Benchmark Repository for Medical Vision-Language Models

üìö A comprehensive summary of recent benchmarks for evaluating and training Medical Vision-Language Models (Med-VLMs)

---

## üë®‚Äçüíª Contributors

- üßë‚Äçüî¨ **Zanting Ye**  
  Southern Medical University  
  üìß yzt2861252880@gmail.com

- üßë‚Äçüî¨ **Xu Han**  
  Shanghai Jiao Tong University  
  üìß hanxv8826@gmail.com

- üßë‚Äçüî¨ **Xiaolong Niu**  
  Southern Medical University

- üßë‚Äçüî¨ **Zian Wang**  
  Shanghai Jiao Tong University
  
- üßë‚Äçüî¨ **Shengyuan Liu**  
  The Chinese University of Hong Kong   
  üìß liushengyuan@link.cuhk.edu.hk
- üë®‚Äçüè´ **Lijun Lu**  
  Southern Medical University
  


---

## üìä GitHub Stats

![Stars](https://img.shields.io/github/stars/yezanting/Med-VLM-Bench-Summary?style=social)
![Forks](https://img.shields.io/github/forks/yezanting/Med-VLM-Bench-Summary?style=social)
![License](https://img.shields.io/github/license/yezanting/Med-VLM-Bench-Summary)
![Last Commit](https://img.shields.io/github/last-commit/yezanting/Med-VLM-Bench-Summary)

---

## üîç Project Overview

With the continuous advancement of research on Medical Vision-Language Models (Med-VLMs) and their reasoning capabilities, a number of high-quality, publicly available datasets focusing on medical reasoning have been released between March and May 2025. These datasets provide a solid foundation for the development of multimodal medical AI systems.

**Med-VLM-Bench** is a curated, continuously updated repository of the latest and most important datasets for training and evaluating medical LLMs and VLMs. This project focuses on:

- ‚úÖ Reasoning-centric multimodal benchmarks
- üìÖ Latest datasets published in Mar‚ÄìMay 2025
- üß† Foundational datasets from 2023‚Äì2024
- üîó Direct access to dataset links or HuggingFace/GitHub repositories

üí° Our knowledge is limited to public sources. We welcome community contributions ‚Äî feel free to open an issue to share new datasets, and we will update promptly. 

üìåNote: The annotation time of the dataset is based on the publication time of the corresponding article.

---

### üì¢ News

#### üåü Latest Updates
- **2025-06-29**: üéâ Added new datasets/benchmarks AbdomenAtlas 3.0 (ICCV2025), Derm1M(ICCV2025), MedTVT-R1, , GEMeX(ICCV2025) and HIE-Reasoning(ICML2025). Check it out for detailed information and download links!
- **2025-06-18**: üéâ Added new datasets/benchmarks Lingshu, ReasonMed. Check it out for detailed information and download links!
- **2025-06-11**: üéâ Added some recent datasets and benchmarks!
- **2025-06-11**: üéâ Create our github project!
## üìä Dataset Summary Table

| Dataset Name       | Paper Title                                                                                     | Year / Venue            | Data Modality              | Task Type                      | Size                          | Download Link                                                                                      |
|--------------------|--------------------------------------------------------------------------------------------------|--------------------------|----------------------------|--------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------|
| MedTVT-QA | [MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis](https://arxiv.org/pdf/2506.18512) | 2025.06.23 | Text + Time Series (ECG) + Image (CXR) + Tabular (Lab Test) | Multimodal Medical Reasoning, Multi-disease Diagnosis, Report Generation | 8,706 multimodal data combinations used to generate QA pairs | [Github](https://github.com/keke-nice/MedTVT-R1) |
| HIE-Reasoning | [Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning](https://openreview.net/pdf?id=tnyxtaSve5) | 2025.06.18(ICML2025) | Text + Image (MRI) + Clinical Data | Professional-level Medical Reasoning, Neurocognitive Outcome Prediction, Lesion Analysis | 133 unique MRIs, 749 professional QA pairs, 133 interpretation summaries | [Github](https://github.com/i3-research/HIE-Reasoning) |
| ReasonMed| [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513) | 2025.06.11 | Text (Multi-agent CoT, Summary, QA) | Medical Reasoning, QA, CoT Fine-tuning | 370K high-quality samples distilled from 1.75M CoT paths, based on 195K questions from 4 benchmarks | [HF](https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed) |
| Lingshu (Train) | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | Text + Image (Multimodal Instruction, VQA, Report) | Multimodal Medical QA, Reasoning, Consultation, Report Generation | ~9.3M training samples from 60+ datasets | [Project Page](https://alibaba-damo-academy.github.io/lingshu/) |
| MedEvalKit (Linshu Test) | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | Text + Image (Multimodal Benchmarks) | Benchmarking: VQA, Report Generation, Medical Text QA | 152,066 evaluation samples from 16 benchmarks | [Github](https://github.com/alibaba-damo-academy/MedEvalKit) |
| MIRIAD | [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091) | 2025.06.09 | Text (Instruction-Response) | Medical QA, Retrieval-Augmented Generation (RAG), Hallucination Detection | 5.8M / 4.4M QA pairs | [HF](https://huggingface.co/datasets/miriad/miriad-5.8M) |
| ClinBench-HPB | [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095) | 2025.06.04 | Text (Multiple-choice Questions, Clinical Cases) | Medical Question Answering, Hepato-Pancreato-Biliary Clinical Case Diagnosis | 3,535 MCQs & 337 clinical cases, covering 465+ Hepato-Pancreato-Biliary diseases | [Project Page](https://clinbench-hpb.github.io), [HF](https://huggingface.co/datasets/ASD9987/ClinBench-HPB) |
| SurgVLM-DB         |SurgVLM-DB: A Large-scale Multimodal Surgical Database Comprising Over 1.81 Million Frames with 7.79 Million Conversations                                                        | 2025.06                 | Video + Text               | Multimodal QA                  | 1.81M frames, 7.79M QAs       | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                   |
| EndoBench    | [EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis](https://arxiv.org/abs/2505.23601v1) | 2025.05.29   | Image + Text (Visual QA, Multimodal Tasks, Multi-level Visual Prompts) | Endoscopy Analysis, Medical Imaging, Multimodal Model Evaluation | Covers 4 endoscopy scenarios (Gastroscopy, Colonoscopy, Capsule Endoscopy, Surgical Endoscopy); includes 12 clinical tasks and 12 subtasks; 5 levels of visual prompt granularity; 6832 clinically validated VQA samples | [HF](https://huggingface.co/datasets/Saint-lsy/EndoBench) |
| MedXpertQA | MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding | ICML2025 | Text + Image (Multimodal MCQs) | Expert-level Medical QA, Clinical Reasoning, Multimodal Understanding | 4,460 questions (2,455 text / 2,005 image) | [HF](https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA) |
| MedCaseReasoning   | MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports                         | 2025.05.20              | Text                       | Diagnostic Reasoning           | 14,489 QA cases               | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                   |
| vlm-project-with-images-with-bbox-images-with-tree-of-thoughts | - | 2025.06                 | MRI Image + BBox + Multilingual QA (Including 7 languages: vi, en, fr, de, zh, ko, ja) | VQA, Lesion Detection, Clinical Reasoning (Tree-of-Thought) | 12.3k samples | [HF](https://huggingface.co/datasets/tungvu3196/vlm-project-with-images-with-bbox-images-with-tree-of-thoughts) | N/A |
| DrVD-Bench         | DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?                | 2025.05.30              | Medical Images + Text      | VQA, Reasoning, Report Gen.    | 7,789 image‚ÄìQA pairs          | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine                                     | 2025.05                 | Text                       | Instruction Tuning             | 5M instances, 19K instructions| [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                            |
| MedS-Bench         | -                                                                                                | 2025.05                 | Text                       | Clinical Task Benchmark        | 11 task types                 | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                          |
| MM-Skin            | MM-Skin:Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks                    | 2025.05.09              | Image + Text               | Open-ended VQA (no reasoning)  | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                        |
| AlphaMed19K        | Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL                                                     | 2025.05.23              | Text                       | QA Reasoning                   | 19K QAs                       | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                           |
| Derm1M | [Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology](https://arxiv.org/pdf/2503.14911) | 2025.04.13(ICCV2025) | Text + Image (Dermatology) | Skin Disease Classification, Concept Identification, Cross-modal Retrieval | 1,029,761 image-text pairs | [Github](https://github.com/SiyuanYan1/Derm1M) |
| GEMeX | [GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis](https://arxiv.org/pdf/2411.16778) | 2025.03.23 (ICCV2025) | Text + Image (Chest X-ray) | Medical Visual Question Answering (VQA) for Chest X-ray Diagnosis | 151,025 images and 1,605,575 QA pairs | [HF](https://huggingface.co/datasets/BoKelvin/GEMeX-VQA), [Github](https://github.com/Awenbocc/GEMeX-Project) |
| Surg-396K | [EndoChat: Grounded Multimodal Large Language Model for Endoscopic Surgery](https://arxiv.org/abs/2501.11347) | 2025.03.15 | Image + Text (Multimodal Instruction, VQA, Grounding, Description) | Endoscopic Surgery, Surgical Scene Understanding, Visual QA, Grounded Dialogue | 396K instruction-image pairs from 41.4K images across 3 datasets (EndoVis, CoPESD, Cholec80) with 5 conversation types and 7 scene understanding tasks | [GitHub](https://github.com/gkw0010/EndoChat), [Data Link](https://mycuhk-my.sharepoint.com/personal/1155180074_link_cuhk_edu_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1155180074%5Flink%5Fcuhk%5Fedu%5Fhk%2FDocuments%2FRENLAB%20AI%20Data%2FSurg%2D396k&ga=1) |
| AbdomenAtlas 3.0 | [RadGPT: Constructing 3D Image-Text Tumor Datasets](https://arxiv.org/pdf/2501.04678) | 2025.01.08(ICCV2025) | Text + 3D Image (Abdominal CT) | 3D Abdominal CT Report Generation, Tumor Segmentation, Staging, and Analysis | 9,262 3D CT scans with paired reports, detailing 8,562 tumor instances | [Github](https://github.com/MrGiovanni/RadGPT), [HF](https://huggingface.co/datasets/SpamYdob/AbdomenAtlas3.0Report)|
| HuatuoGPT-o1 Dataset | [HuatuoGPT-o1ÔºåTowards Medical Complex Reasoning with LLMs](https://arxiv.org/pdf/2412.18925) | 2024.12.25 | Text (Complex CoT, Medical Verifiable Problems, Multi-Step Reasoning) | Medical Complex Reasoning, CoT Fine-tuning, Reinforcement Learning | Contains 40K high-quality medical complex reasoning problems filtered by a medical verifier, based on MedQA-USMLE and MedMCQA medical exam training sets | [GitHub](https://github.com/FreedomIntelligence/HuatuoGPT-o1), [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) |
| PubMedVision        | [HuatuoGPT-Visionn, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale](https://arxiv.org/abs/2406.19280)                         | 2024.09.30 | Image + Text (Multimodal) | Medical VQA (Alignment VQA, Instruction-Tuning VQA), Captioning, Summarization | 1.3M VQA samples from 914,960 filtered PubMed medical images & text (647K + 647K)     | [Hugging Face](https://huggingface.co/datasets/FreedomIntelligence/PubMedVision)|
| PMC-VQA            | PMC-VQA: Visual Instruction Tuning for Medical VQA                                                       | 2024.08.08              | Image + Text               | VQA                            | 226,946 QA pairs              | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                            |
| VQARad             | - | 2023.08.07              | Radiography                | VQA                            | 315 images, 3,515 QAs         | [OSF](https://osf.io/89kps/)                                                                         |
|  Asclepius | [Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2402.11217) | ACL 2025 | Image + Text | VQA | 3232 VQA pairs encompassing 15 medical specialties, stratifying into 3 main categories and 8 sub-categories of clinical tasks | [GitHub](https://github.com/Asclepius-Med/Asclepius) |
|  MedTrinity-25M | [MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine](https://yunfeixie233.github.io/MedTrinity-25M/) | ICLR 2025 | Image + Text | VQA | 25M VQA pairs, covering over 25 million images across 10 modalities | [HF](https://huggingface.co/datasets/UCSC-VLAA/MedTrinity-25M) |
|  MediConfusion | [MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models](https://arxiv.org/abs/2409.15477) | ICLR 2025 | Image + Text | VQA | 176 confusing pairs, a set of two images that share the same question and corresponding answer options, but the correct answer is different for the images. | [HF](https://huggingface.co/datasets/shahab7899/MediConfusion) |
| GMAI-MMBench       | GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI                     | NeurIPS 2024            | Multi-modal (38 types)     | VQA                            | 26K QA pairs                  | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                       |
| PathMMU            | PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology       | 2024.03.20              | Pathology Image + Text     | Multi-choice, Reasoning        | 33,428 QAs, 24,067 images     | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                             |
| OmniMedVQA         | OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM                          | CVPR 2024               | Multi-modal (12 types)     | VQA                            | 118,010 images, 127,995 QA    | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                       |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models                 | NeurIPS 2024            | Medical Images + QA        | Open/Closed QA                 | 41K QA pairs                  | [GitHub](https://github.com/richard-peng-xia/CARES)                                                |
| MultiMedEval       | MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models                        | 2024.02.16              | Image + Text               | Multi-task Evaluation          | 6 tasks, 23 datasets          | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                             |
| PubMedVision       | HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale | EMNLP 2024          | Medical Images + QA            | Medical VQA                         | 1,294,062 QA pairs                      | [HF](https://huggingface.co/datasets/FreedomIntelligence/PubMedVision)                                     |
| medical-o1-reasoning-SFT       | HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs | ACL 2025          | Medical VQA„ÄÅReasoning            | Medical VQA                         | 19.7k QA pairs                      | [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT)                                     |

---

## üá®üá≥ ‰∏≠ÊñáÁâàÊú¨

### üîç È°πÁõÆÁÆÄ‰ªã

ÈöèÁùÄÂåªÂ≠¶ËßÜËßâËØ≠Ë®ÄÊ®°ÂûãÔºàMed-VLMÔºâÂèäÂÖ∂Êé®ÁêÜËÉΩÂäõÁ†îÁ©∂ÁöÑÊåÅÁª≠Êé®ËøõÔºåÂ∞§ÂÖ∂Âú® 2025 Âπ¥ 3 ÊúàËá≥ 5 ÊúàÊúüÈó¥ÔºåÈôÜÁª≠ÂèëÂ∏É‰∫Ü‰ºóÂ§öÈ´òË¥®Èáè„ÄÅËÅöÁÑ¶‰∫éÂåªÂ≠¶Êé®ÁêÜËÉΩÂäõÁöÑÊñ∞ÂûãÂÖ¨ÂºÄÊï∞ÊçÆÈõÜÔºå‰∏∫Â§öÊ®°ÊÄÅÂåªÁñó‰∫∫Â∑•Êô∫ËÉΩÁöÑÂèëÂ±ïÊèê‰æõ‰∫ÜÂùöÂÆûÁöÑÊï∞ÊçÆÂü∫Á°Ä„ÄÇ‰∏∫Ê≠§ÔºåÊàë‰ª¨Â∏åÊúõÂ∞ΩÂèØËÉΩÊ±áÊÄªËøô‰∏Ä‰∫õÊï∞ÊçÆÔºåÊúüÂæÖËÉΩ‰∏∫ËØ•Á§æÂå∫Êèê‰æõÊõ¥‰æøÊç∑ÁöÑÊï∞ÊçÆËÆøÈóÆÊñπÂºè„ÄÇÊàë‰ª¨ÂèëÂ∏É‰∫ÜMed-VLM-BenchÔºö

**Med-VLM-Bench** Ëá¥Âäõ‰∫éÊ±áÊÄªÂπ∂Êï¥ÁêÜËøô‰∫õÊ®°ÂûãËÆ≠ÁªÉ‰∏éËØÑ‰º∞ÁöÑÂÖ≥ÈîÆËµÑÊ∫êÔºö

- ‚úÖ ËÅöÁÑ¶ 2025 Âπ¥ 3‚Äì6 ÊúàÂèëÂ∏ÉÁöÑÊñ∞Êï∞ÊçÆÈõÜ  
- üß† ÈáçÁÇπÂº∫Ë∞ÉÊé®ÁêÜËÉΩÂäõ„ÄÅÂ§öÊ®°ÊÄÅÁêÜËß£ÂíåÈóÆÁ≠îËÉΩÂäõÁöÑÊï∞ÊçÆÈõÜ 
- üß™ ÂêåÊó∂Ë¶ÜÁõñ 2023‚Äì2024 Âπ¥ÁöÑÁªèÂÖ∏Med LLM/VLM benchmark datasets
- üîó Êèê‰æõÁõ¥Êé•ÂèØÁî®ÁöÑ‰∏ãËΩΩÈìæÊé•ÂíåÂºÄÊ∫êÂú∞ÂùÄ

üí° Êàë‰ª¨ÁöÑÁü•ËØÜÊù•Ê∫êÊúâÈôêÔºåÊ¨¢ËøéÂ§ßÂÆ∂ÈÄöËøá Issue Êàñ PR Êé®ËçêÊõ¥Â§öÊï∞ÊçÆÈõÜÔºåÊàë‰ª¨‰ºöÁ¨¨‰∏ÄÊó∂Èó¥Êõ¥Êñ∞ÔºÅ

üìåNote: Ê≠§Â§ñÊàë‰ª¨ÁöÑÊï∞ÊçÆÈõÜÊ†áÊ≥®Êó∂Èó¥‰ª•Áõ∏Â∫îÊñáÁ´†ÂèëË°®Êó∂Èó¥‰∏∫ÂáÜ

---

### üìä Êï∞ÊçÆÈõÜÊ±áÊÄªË°®

| Êï∞ÊçÆÈõÜÂêçÁß∞         | ËÆ∫ÊñáÊ†áÈ¢ò                                                                 | Âπ¥‰ªΩ / ‰ºöËÆÆ              | Êï∞ÊçÆÊ®°ÊÄÅ                  | ‰ªªÂä°Á±ªÂûã                        | Êï∞ÊçÆËßÑÊ®°                       | ‰∏ãËΩΩÈìæÊé•                                                                                         |
|--------------------|--------------------------------------------------------------------------|---------------------------|---------------------------|----------------------------------|-------------------------------|--------------------------------------------------------------------------------------------------|
| MedTVT-QA | [MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis](https://arxiv.org/pdf/2506.18512) | 2025.06.23 | ÊñáÊú¨ + Êó∂Èó¥Â∫èÂàó (ÂøÉÁîµÂõæ) + ÂõæÂÉè (ËÉ∏ÈÉ®XÂÖâ) + Ë°®Ê†º (Ë°ÄÊ∂≤Ê£ÄÊµã) | Â§öÊ®°ÊÄÅÂåªÁñóÊé®ÁêÜ„ÄÅÂ§öÁóÖÁßçËØäÊñ≠„ÄÅÊä•ÂëäÁîüÊàê | Áî®‰∫éÁîüÊàêQAÂØπÁöÑ8,706ÁªÑÂ§öÊ®°ÊÄÅÊï∞ÊçÆÁªÑÂêà | [Github](https://github.com/keke-nice/MedTVT-R1) |
| HIE-Reasoning | [Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning](https://openreview.net/pdf?id=tnyxtaSve5) | 2025.06.18(ICML2025) | ÊñáÊú¨ + ÂõæÂÉè (MRI) + ‰∏¥Â∫äÊï∞ÊçÆ | ‰∏ì‰∏öÁ∫ßÂåªÁñóÊé®ÁêÜ„ÄÅÁ•ûÁªèËÆ§Áü•ÁªìÂ±ÄÈ¢ÑÊµã„ÄÅÁóÖÁÅ∂ÂàÜÊûê | 133‰∏™Áã¨Á´ãMRI„ÄÅ749‰∏™‰∏ì‰∏öÈóÆÁ≠îÂØπ„ÄÅ133‰ªΩËß£ËØªÊëòË¶Å | [Github](https://github.com/i3-research/HIE-Reasoning) |
| ReasonMed | [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513) | 2025.06.11 | ÊñáÊú¨ÔºàÂ§ö‰ª£ÁêÜÊé®ÁêÜ„ÄÅÂ§öÊ≠•ÊÄªÁªì„ÄÅÂåªÂ≠¶ÈóÆÁ≠îÔºâ | ÂåªÂ≠¶Êé®ÁêÜ„ÄÅÈóÆÁ≠î„ÄÅÈìæÂºèÊÄùÁª¥ÂæÆË∞É | ‰ªé 175 ‰∏áÊù° CoT Ë∑ØÂæÑ‰∏≠Á≤æÁÇºÂá∫ÁöÑ 37 ‰∏áÈ´òË¥®ÈáèÊ†∑Êú¨ÔºåË¶ÜÁõñÊù•Ëá™ 4 ‰∏™Âü∫ÂáÜÁöÑ 19.5 ‰∏áÈóÆÈ¢ò | [HF](https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed) |
| LingshuÔºàTrainÔºâ | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | ÊñáÊú¨ + ÂõæÂÉèÔºàÂ§öÊ®°ÊÄÅÊåá‰ª§„ÄÅVQA„ÄÅÊä•ÂëäÔºâ | Â§öÊ®°ÊÄÅÂåªÂ≠¶ÈóÆÁ≠î„ÄÅÊé®ÁêÜ„ÄÅÈóÆËØä„ÄÅÊä•ÂëäÁîüÊàê | Á∫¶ 930 ‰∏áËÆ≠ÁªÉÊ†∑Êú¨ÔºåÊù•Ëá™ 60+ Êï∞ÊçÆÈõÜ | [Project Page](https://alibaba-damo-academy.github.io/lingshu/) |
| MedEvalKitÔºàLinshu testÔºâ | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | ÊñáÊú¨ + ÂõæÂÉèÔºàÂ§öÊ®°ÊÄÅËØÑÊµãÂü∫ÂáÜÔºâ | Âü∫ÂáÜËØÑÊµãÔºöVQA„ÄÅÊä•ÂëäÁîüÊàê„ÄÅÂåªÂ≠¶ÊñáÊú¨ÈóÆÁ≠î | ÂÖ± 152,066 ‰∏™ËØÑ‰º∞Ê†∑Êú¨ÔºåÊù•Ëá™ 16 ‰∏™Âü∫ÂáÜÊï∞ÊçÆÈõÜ | [Github](https://github.com/alibaba-damo-academy/MedEvalKit) |
| MIRIAD | [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091) | 2025.06.09 | ÊñáÊú¨ÔºàÊåá‰ª§-ÂõûÁ≠îÂØπÔºâ | ÂåªÂ≠¶ÈóÆÁ≠î„ÄÅRAG Ê£ÄÁ¥¢Â¢ûÂº∫„ÄÅÂπªËßâÊ£ÄÊµã |  582 ‰∏á / 448 ‰∏á | [HF](https://huggingface.co/datasets/miriad/miriad-5.8M) |
| ClinBench-HPB | [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095) | 2025.06.04 | ÊñáÊú¨ (ÈÄâÊã©È¢ò, ‰∏¥Â∫äÁóÖ‰æã) | ÂåªÂ≠¶ÈóÆÁ≠î, ‰∏¥Â∫äÁóÖ‰æãËØäÊñ≠ | 3,535ÈÅìÈÄâÊã©È¢òÂíå337‰∏™‰∏¥Â∫äÁóÖ‰æã, Ë¶ÜÁõñ465+ÁßçËÇùËÉÜËÉ∞ÁñæÁóÖ | [Project Page](https://clinbench-hpb.github.io), [HF](https://huggingface.co/datasets/ASD9987/ClinBench-HPB) |
| SurgVLM-DB         |SurgVLM-DB: A Large-Scale Multimodal Surgical Database                               | 2025.06                   | ËßÜÈ¢ë + ÊñáÊú¨               | Â§öÊ®°ÊÄÅÈóÆÁ≠î                      | 1.81MÂ∏ß, 7.79MÂØπËØù             | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                 |
| EndoBench  | [EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis](https://arxiv.org/abs/2505.23601v1) | 2025.05.29 | ÂõæÂÉè+ÊñáÊú¨ÔºàËßÜËßâÈóÆÁ≠î„ÄÅÂ§öÊ®°ÊÄÅ‰ªªÂä°„ÄÅÂ§öÂ±ÇÊ¨°ËßÜËßâÊèêÁ§∫Ôºâ | ÂÜÖÈïúÂàÜÊûê„ÄÅÂåªÂ≠¶ÂΩ±ÂÉè„ÄÅÂ§öÊ®°ÊÄÅÊ®°ÂûãËØÑ‰º∞ | Ë¶ÜÁõñËÉÉÈïú„ÄÅÁªìËÇ†Èïú„ÄÅËÉ∂ÂõäÂÜÖÈïúÂíåÊâãÊúØÂÜÖÈïú 4 Â§ßÂú∫ÊôØÔºõÂåÖÂê´ 12 ‰∏™‰∏¥Â∫ä‰ªªÂä°Âèä 12 ‰∏™Ê¨°‰ªªÂä°Ôºõ5 ÁßçËßÜËßâÊèêÁ§∫Á≤íÂ∫¶Ôºõ6832 ‰∏™ÁªèËøá‰∏¥Â∫äÈ™åËØÅÁöÑ VQA Ê†∑Êú¨ | [HF](https://huggingface.co/datasets/Saint-lsy/EndoBench) |
| MedXpertQA | MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding | ICML2025 | ÊñáÊú¨ + ÂõæÂÉèÔºàÂ§öÊ®°ÊÄÅÈÄâÊã©È¢òÔºâ | ‰∏ìÂÆ∂Á∫ßÂåªÂ≠¶ÈóÆÁ≠î„ÄÅ‰∏¥Â∫äÊé®ÁêÜ„ÄÅÂ§öÊ®°ÊÄÅÁêÜËß£ | ÂÖ± 4,460 È¢òÔºàÊñáÊú¨ 2,455 / ÂõæÂÉè 2,005Ôºâ | [HF](https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA) |
| MedCaseReasoning   |MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports  | 2025.05.20                | ÊñáÊú¨                      | ËØäÊñ≠Êé®ÁêÜ                        | 14,489ÈóÆÁ≠îÂØπ                   | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                 |
| vlm-project-with-images-with-bbox-images-with-tree-of-thoughts | - | 2025.06 | MRI ÂõæÂÉè + BBox + Â§öËØ≠ÁßçÈóÆÁ≠îÔºàvi, en, fr, de, zh, ko, jaÔºâ | ÂåªÂ≠¶VQA„ÄÅÁóÖÁÅ∂Ê£ÄÊµã„ÄÅ‰∏¥Â∫äÊé®ÁêÜÔºàTree-of-ThoughtÔºâ | 12,325 Êù°Ê†∑Êú¨ | [HF](https://huggingface.co/datasets/tungvu3196/vlm-project-with-images-with-bbox-images-with-tree-of-thoughts) | Êú™Ê≥®Êòé |
| DrVD-Bench         | DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis? | 2025.05.30          | ÂåªÂ≠¶ÂõæÂÉè + ÊñáÊú¨           | ÂåªÂ≠¶VQA„ÄÅÊé®ÁêÜ„ÄÅÊä•ÂëäÁîüÊàê         | 7,789ÂõæÊñáQAÂØπ                  | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine              | 2025.05                   | ÊñáÊú¨                      | Êåá‰ª§ÂæÆË∞É                        | 5MÊ†∑Êú¨, 19KÊåá‰ª§                | [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                         |
| MedS-Bench         | -                                                                         | 2025.05                   | ÊñáÊú¨                      | ‰∏¥Â∫ä‰ªªÂä°ËØÑ‰º∞                    | 11Â§ßÁ±ª‰ªªÂä°                     | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                       |
| MM-Skin            | MM-Skin:Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks | 2025.05.09          | ÂõæÂÉè + ÊñáÊú¨               | ÂºÄÊîæÂºèVQAÔºàÊó†Êé®ÁêÜÔºâ             | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                      |
| AlphaMed19K        | Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL                              | 2025.05.23                | ÊñáÊú¨                      | Êé®ÁêÜÈóÆÁ≠î                        | 19KÈóÆÁ≠îÂØπ                      | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                         |
| Derm1M | [Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology](https://arxiv.org/pdf/2503.14911) | 2025.04.13(ICCV2025) | ÊñáÊú¨ + ÂõæÂÉèÔºàÁöÆËÇ§ÁóÖÂ≠¶Ôºâ | ÁöÆËÇ§ÁóÖÂàÜÁ±ª„ÄÅÊ¶ÇÂøµËØÜÂà´„ÄÅË∑®Ê®°ÊÄÅÊ£ÄÁ¥¢ | 1,029,761‰∏™ÂõæÊñáÂØπ | [Github](https://github.com/SiyuanYan1/Derm1M) |
| GEMeX | [GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis](https://arxiv.org/pdf/2411.16778) | 2025.03.23 (ICCV2025) | ÊñáÊú¨ + ÂõæÂÉèÔºàËÉ∏ÈÉ®XÂÖâÁâáÔºâ | Áî®‰∫éËÉ∏ÈÉ®XÂÖâËØäÊñ≠ÁöÑÂåªÁñóËßÜËßâÈóÆÁ≠îÔºàVQAÔºâ | 151,025Âº†ÂõæÁâáÂíå1,605,575‰∏™ÈóÆÁ≠î | [HF](https://huggingface.co/datasets/BoKelvin/GEMeX-VQA), [Github](https://github.com/Awenbocc/GEMeX-Project) |
| Surg-396K | [EndoChat: Grounded Multimodal Large Language Model for Endoscopic Surgery](https://arxiv.org/abs/2501.11347) | 2025.03.15 | ÂõæÂÉè + ÊñáÊú¨ÔºàÂ§öÊ®°ÊÄÅÊåá‰ª§„ÄÅÈóÆÁ≠î„ÄÅÁõÆÊ†áÂÆö‰Ωç„ÄÅËØ¶ÁªÜÊèèËø∞Ôºâ | ÂÜÖÁ™•ÈïúÂ§ñÁßë„ÄÅÊâãÊúØÂú∫ÊôØÁêÜËß£„ÄÅËßÜËßâÈóÆÁ≠î„ÄÅÂÆö‰ΩçÂØπËØù | Êù•Ëá™ EndoVis„ÄÅCoPESD Âíå Cholec80 ÁöÑ 41,400 Âº†ÂõæÂÉèÔºåÁîüÊàê 396,000 ÂõæÊñáÂØπÔºåË¶ÜÁõñ 5 ÁßçÂØπËØùÁ±ªÂûã‰∏é 7 Á±ªÊâãÊúØÁêÜËß£‰ªªÂä° | [GitHub](https://github.com/gkw0010/EndoChat) [Data link](https://mycuhk-my.sharepoint.com/personal/1155180074_link_cuhk_edu_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1155180074%5Flink%5Fcuhk%5Fedu%5Fhk%2FDocuments%2FRENLAB%20AI%20Data%2FSurg%2D396k&ga=1)|
| AbdomenAtlas 3.0 | [RadGPT: Constructing 3D Image-Text Tumor Datasets](https://arxiv.org/pdf/2501.04678) | 2025.01.08 (ICCV2025) | ÊñáÊú¨ + 3DÂõæÂÉè (ËÖπÈÉ®CT) | 3DËÖπÈÉ®CTÊä•ÂëäÁîüÊàê„ÄÅËÇøÁò§ÂàÜÂâ≤„ÄÅÂàÜÊúü‰∏éÂàÜÊûê | 9,262ÁªÑ3D CTÊâ´ÊèèÂèäÈÖçÂØπÊä•ÂëäÔºåÂåÖÂê´8,562‰∏™ËÇøÁò§ÂÆû‰æã | [Github](https://github.com/MrGiovanni/RadGPT), [HF](https://huggingface.co/datasets/SpamYdob/AbdomenAtlas3.0Report)|
| HuatuoGPT-o1 Dataset | [HuatuoGPT-o1ÔºåTowards Medical Complex Reasoning with LLMs](https://arxiv.org/pdf/2412.18925) | 2024.12.25 | ÊñáÊú¨ÔºàÂ§çÊùÇÈìæÂºèÊÄùÁª¥„ÄÅÂåªÂ≠¶È™åËØÅÈ¢ò„ÄÅÂ§öÊ≠•Êé®ÁêÜÔºâ | ÂåªÂ≠¶Â§çÊùÇÊé®ÁêÜ„ÄÅÈìæÂºèÊÄùÁª¥ÂæÆË∞É„ÄÅÂº∫ÂåñÂ≠¶‰π† | ÂåÖÂê´ 40K ÁªèÂåªÂ≠¶È™åËØÅÂô®Á≠õÈÄâÁöÑÈ´òË¥®ÈáèÂåªÂ≠¶Â§çÊùÇÊé®ÁêÜÈóÆÈ¢òÔºåÂü∫‰∫é MedQA-USMLE Âíå MedMCQA ÂåªÂ≠¶ËÄÉËØïËÆ≠ÁªÉÈõÜ | [GitHub](https://github.com/FreedomIntelligence/HuatuoGPT-o1), [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) |
| PubMedVision         | [HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale](https://arxiv.org/abs/2406.19280)                         | 2024.09.30 | ÂõæÂÉè + ÊñáÊú¨ÔºàÂ§öÊ®°ÊÄÅÔºâ     | ÂåªÂ≠¶ËßÜËßâÈóÆÁ≠îÔºàVQAÔºâ„ÄÅÂõæÊñáÂØπÈΩê„ÄÅÊåá‰ª§ÂæÆË∞É„ÄÅÊèèËø∞ÁîüÊàêÁ≠â                         | 130 ‰∏á VQA Ê†∑Êú¨ÔºåÊù•Ëá™ PubMed ‰∏≠Á≠õÈÄâÁöÑ 91.5 ‰∏áÂåªÂ≠¶ÂõæÂÉè‰∏é‰∏ä‰∏ãÊñáÔºà647K + 647KÔºâ        | [HF](https://huggingface.co/datasets/FreedomIntelligence/PubMedVision)      |
| PMC-VQA            | PMC-VQA: Visual Instruction Tuning for Medical VQA                                 | 2024.08.08                | ÂõæÂÉè + ÊñáÊú¨               | ÂåªÂ≠¶VQA                         | 226,946ÈóÆÁ≠îÂØπ                  | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                          |
| VQARad             |-| 2023.08.07                | ÊîæÂ∞ÑÂõæÂÉè                  | VQA                             | 315ÂõæÂÉè, 3515ÈóÆÁ≠î              | [OSF](https://osf.io/89kps/)                                                                       |
|  Asclepius | [Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2402.11217) | ACL 2025 | ÂõæÂÉè + ÊñáÊú¨ | ÂåªÂ≠¶VQA | 3232Êù°ÈóÆÁ≠îÂØπÔºåÊ∂µÁõñ 15 ‰∏™ÂåªÂ≠¶‰∏ì‰∏öÔºåÂàÜ‰∏∫ 3 ‰∏™‰∏ªË¶ÅÁ±ªÂà´Âíå 8 ‰∏™Â≠êÁ±ªÂà´ÁöÑ‰∏¥Â∫ä‰ªªÂä° | [GitHub](https://github.com/Asclepius-Med/Asclepius) |
|  MedTrinity-25M | [MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine](https://yunfeixie233.github.io/MedTrinity-25M/) | ICLR 2025 | ÂõæÂÉè + ÊñáÊú¨ | ÂåªÂ≠¶VQA | Â§ßËßÑÊ®°ÂåªÂ≠¶Â§öÊ®°ÊÄÅÊï∞ÊçÆÈõÜÔºåÊ∂µÁõñ 10 ÁßçÊ®°ÊÄÅÁöÑ2500‰∏áÂº†ÂõæÂÉèÔºå‰∏∫65ÁßçÁñæÁóÖÊèê‰æõÂ§öÁ≤íÂ∫¶Ê≥®Èáä | [HF](https://huggingface.co/datasets/UCSC-VLAA/MedTrinity-25M) |
|  MediConfusion | [MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models](https://arxiv.org/abs/2409.15477) | ICLR 2025 | ÂõæÂÉè + ÊñáÊú¨ | ÂåªÂ≠¶VQA | Áî± 176 ‰∏™‰ª§‰∫∫Âõ∞ÊÉëÁöÑÂØπÁªÑÊàê„ÄÇÊ∑∑Ê∑ÜÂØπÊòØ‰∏ÄÁªÑ‰∏§Âº†ÂõæÂÉèÔºåÂÆÉ‰ª¨ÂÖ±‰∫´Áõ∏ÂêåÁöÑÈóÆÈ¢òÂíåÁõ∏Â∫îÁöÑÁ≠îÊ°àÈÄâÈ°πÔºå‰ΩÜÂõæÂÉèÁöÑÊ≠£Á°ÆÁ≠îÊ°à‰∏çÂêå„ÄÇ | [HF](https://huggingface.co/datasets/shahab7899/MediConfusion) |
| GMAI-MMBench       | GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI | NeurIPS 2024          | Â§öÊ®°ÊÄÅÔºà38ÁßçÔºâ            | ÂåªÂ≠¶VQA                         | 26KÈóÆÁ≠îÂØπ                      | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                     |
| PathMMU            | PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Patholog | 2024.03.20      | ÁóÖÁêÜÂõæÂÉè + ÊñáÊú¨           | ÈÄâÊã©È¢ò+Êé®ÁêÜ                     | 33,428ÈóÆÁ≠î, 24,067ÂõæÂÉè         | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                           |
| OmniMedVQA         | OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM    | CVPR 2024                 | Â§öÊ®°ÊÄÅÔºà12ÁßçÔºâ            | ÂåªÂ≠¶VQA                         | 118,010ÂõæÂÉè, 127,995ÈóÆÁ≠î       | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                     |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models | NeurIPS 2024         | ÂåªÂ≠¶ÂõæÂÉè+ÈóÆÁ≠î             | ÂºÄÊîæ‰∏éÂ∞ÅÈó≠ÈóÆÁ≠î                   | 41KÈóÆÁ≠îÂØπ                      | [GitHub](https://github.com/richard-peng-xia/CARES)                                              |
| MultiMedEval       | MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models  | 2024.02.16                | ÂõæÊñáÂ§öÊ®°ÊÄÅ                | Â§ö‰ªªÂä°ËØÑ‰º∞                      | 6‰ªªÂä°, 23Êï∞ÊçÆÈõÜ                | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                           |
| PubMedVision       | HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale | EMNLP 2024          | ÂåªÂ≠¶ÂõæÂÉè + ÊñáÊú¨            | ÂåªÂ≠¶VQA                         | 1,294,062Êù°Êï∞ÊçÆÔºàÂØπÈΩêÊï∞ÊçÆ:647,031ÔºåÊåá‰ª§ÂæÆË∞ÉÊï∞ÊçÆ:647,031 Ôºâ                      | [HF](https://huggingface.co/datasets/FreedomIntelligence/PubMedVision)                                     |
| medical-o1-reasoning-SFT       | HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs | ACL 2025          | ÂåªÂ≠¶VQA„ÄÅÊé®ÁêÜ            | ÂåªÂ≠¶VQA                         | 19.7kÈóÆÁ≠îÂØπ                      | [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT)                                     |

---

## üì¨ ËÅîÁ≥ªÊàë‰ª¨ / Issues & Contact

Â¶ÇÊúâ‰ªª‰ΩïÈóÆÈ¢òÊ¨¢ËøéÊèê‰∫§ Issue ÊàñÈÄöËøáÈÇÆ‰ª∂ËÅîÁ≥ªÔºö

- üìß **Âè∂ËµûÊå∫ (Zanting Ye)**: yzt2861252880@gmail.com  
- üìß **Èü©Áª™ (Xu Han)**: hanxv8826@gmail.com

---

## üë• Âêà‰ΩúËÄÖÔºàContributorsÔºâ

<table>
  <tr>
    <td align="center">
      <a href="https://github.com/yezanting">
        <img src="https://github.com/yezanting.png" width="80px;" alt="yezanting"/><br />
        <sub><b>Âè∂ËµûÊå∫(Zanting Ye)</b></sub><br />
        <a href="mailto:yzt2861252880@gmail.com">üìß</a>
      </a>
    </td>
    <td align="center">
      <a href="https://github.com/lailainan">
        <img src="https://github.com/lailainan.png" width="80px;" alt="lailainan"/><br />
        <sub><b>Èü©Áª™(Xu Han)</b></sub><br />
        <a href="mailto:hanxv8826@gmail.com">üìß</a>
      </a>
    </td>
        <td align="center">
          <a href="https://github.com/Long0121">
          <img src="https://github.com/Long0121.png" width="80px;" alt="lailainan"/><br />
          <sub><b>ÁâõÂ∞èÈæô(Xiaolong Niu)</b></sub><br />
          <a href="">üìß</a>
     </td>
        <td align="center">
          <a href="https://github.com/zianwang1110">
          <img src="https://github.com/zianwang1110.png" width="80px;" alt="lailainan"/><br />
          <sub><b>ÁéãÊ¢ìÂÆâ(Zian Wang)</b></sub><br />
          <a href="">üìß</a>  
      </a>
    </td>
        <td align="center">
          <a href="https://github.com/Saint-lsy">
          <img src="https://github.com/Saint-lsy.png" width="80px;" alt="lailainan"/><br />
          <sub><b>ÂàòÂú£ÂúÜ(Shengyuan Liu)</b></sub><br />
          <a href="liushengyuan@link.cuhk.edu.hk">üìß</a> 
      </a>
    </td>
  </tr>
</table>

---

## ‚≠ê Star Ë∂ãÂäøÂõæ (Star History)

![Star History Chart](https://api.star-history.com/svg?repos=yezanting/Med-VLM-Bench-Summary&type=Date)

---

---

‚≠ê Star Êú¨È°πÁõÆÊîØÊåÅÊàë‰ª¨ÊåÅÁª≠Êõ¥Êñ∞ÔºåÊ¨¢Ëøé PR ÂíåÂª∫ËÆÆ‰∫§ÊµÅÔºÅ
