# ğŸ§  Med-VLM-Bench: A Curated Benchmark Repository for Medical Vision-Language Models

ğŸ“š A comprehensive summary of recent benchmarks for evaluating and training Medical Vision-Language Models (Med-VLMs)

---

## ğŸ‘¨â€ğŸ’» Contributors

- ğŸ§‘â€ğŸ”¬ **Zanting Ye**  
  Southern Medical University  
  ğŸ“§ yzt2861252880@gmail.com

- ğŸ§‘â€ğŸ”¬ **Xu Han**  
  Shanghai Jiao Tong University  
  ğŸ“§ hanxv8826@gmail.com

- ğŸ§‘â€ğŸ”¬ **Xiaolong Niu**  
  Southern Medical University

- ğŸ§‘â€ğŸ”¬ **Zian Wang**  
  Shanghai Jiao Tong University
  
- ğŸ§‘â€ğŸ”¬ **Shengyuan Liu**  
  The Chinese University of Hong Kong   
  ğŸ“§ liushengyuan@link.cuhk.edu.hk
  
- ğŸ§‘â€ğŸ”¬ **Xin Liu**  
  Southern Medical University    
  ğŸ“§ lx10230114@gmail.com
  
- ğŸ‘¨â€ğŸ« **Lijun Lu**  
  Southern Medical University
  


---

## ğŸ“Š GitHub Stats

![Stars](https://img.shields.io/github/stars/yezanting/Med-VLM-Bench-Summary?style=social)
![Forks](https://img.shields.io/github/forks/yezanting/Med-VLM-Bench-Summary?style=social)
![License](https://img.shields.io/github/license/yezanting/Med-VLM-Bench-Summary)
![Last Commit](https://img.shields.io/github/last-commit/yezanting/Med-VLM-Bench-Summary)

---

## ğŸ” Project Overview

With the continuous advancement of research on Medical Vision-Language Models (Med-VLMs) and their reasoning capabilities, a number of high-quality, publicly available datasets focusing on medical reasoning have been released between March and May 2025. These datasets provide a solid foundation for the development of multimodal medical AI systems.

**Med-VLM-Bench** is a curated, continuously updated repository of the latest and most important datasets for training and evaluating medical LLMs and VLMs. This project focuses on:

- âœ… Reasoning-centric multimodal benchmarks
- ğŸ“… Latest datasets published in Mar 2025â€“2026
- ğŸ§  Foundational datasets from 2023â€“2024
- ğŸ”— Direct access to dataset links or HuggingFace/GitHub repositories

ğŸ’¡ Our knowledge is limited to public sources. We welcome community contributions â€” feel free to open an issue to share new datasets, and we will update promptly. 

ğŸ“ŒNote: The annotation time of the dataset is based on the publication time of the corresponding article.

---

### ğŸ“¢ News

#### ğŸŒŸ Latest Updates
- **2026-01-21**: ğŸ‰ Added some recent datasets and benchmarks!Check it out for detailed information and download links!
- **2025-06-29**: ğŸ‰ Added new datasets/benchmarks AbdomenAtlas 3.0 (ICCV2025), Derm1M(ICCV2025), MedTVT-R1, , GEMeX(ICCV2025) and HIE-Reasoning(ICML2025). Check it out for detailed information and download links!
- **2025-06-18**: ğŸ‰ Added new datasets/benchmarks Lingshu, ReasonMed. Check it out for detailed information and download links!
- **2025-06-11**: ğŸ‰ Added some recent datasets and benchmarks!
- **2025-06-11**: ğŸ‰ Create our github project!
## ğŸ“Š Dataset Summary Table

| Dataset Name       | Paper Title                                                                                     | Year / Venue            | Data Modality              | Task Type                      | Size                          | Download Link                                                                                      |
|--------------------|--------------------------------------------------------------------------------------------------|--------------------------|----------------------------|--------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------|
| Multi-RADS                       | [Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models](https://arxiv.org/pdf/2601.03232)         | 2026.1.6                                                                | Text (Synthetic Reports)      | RADS Classification          | 1,600 synthetic reports covering 17 distinct imaging findings.             | [Github](https://github.com/RadioX-Labs/RADSet)                                               |
| Bones and Joints (B&J) Benchmark | [The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision Language Models for Clinical Competency](https://arxiv.org/pdf/2512.22275)           | 2025.12.25                                                              | Text & Image (X-ray, CT, MRI) | VQA & Treatment Planning     | 1,245 question-answer pairs spanning 7 clinical competency tasks.          | [ Hugging Face](https://huggingface.co/datasets/PUTH2025/Bones_and_Joints_Benchmark)          |
| MediEval                         | [MediEval: A Unified Medical Benchmark for Patient-Contextual and Knowledge-Grounded Reasoning in LLMs](https://arxiv.org/pdf/2512.20822)                                 | 2025.12.23                                                              | Text (EHR & Notes)            | Natural Language Inference   | 37,144 medical statements derived from 2,015 hospital admissions.          | [GitHub](https://anonymous.4open.science/r/MediEval-5FA5/)<br>                                |
| TCM-BEST4SDT                     | [A benchmark dataset for evaluating Syndrome Differentiation and Treatment in large language models](https://arxiv.org/pdf/2512.02816)                                    | 2025.12.2                                                               | Text (TCM Case Reports)       | Syndrome Differentiation     | 600 total questions including 300 clinical syndrome differentiation cases. | [GitHub](https://github.com/DYJG-research/TCM-BEST4SDT)                                       |
| SurgMLLMBench                    | [SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding](https://arxiv.org/pdf/2511.21339)                                   | 2025.11.26                                                              | Video & Text                  | Surgical Scene Understanding | 10,652 frames with annotations integrated from 5 surgical datasets.        | [project page](https://surgmllmbench.github.io/)                                              |
| MedVision                        | [MedVision: Dataset and Benchmark for Quantitative Medical Image Analysis](https://arxiv.org/pdf/2511.18676)                                                              | 2025.11.24                                                              | Image (CT, MRI, X-ray, PET)   | Detection & Measurement      | 30.8 million image-annotation pairs across 22 public datasets.             | [project page](https://medvision-vlm.github.io/)                                              |
| EHRStruct                        | [EHRStruct: A Comprehensive Benchmark Framework for Evaluating Large Language Models on Structured Electronic Health Record Tasks](https://arxiv.org/pdf/2511.08206)      | 2025.12.1                                                               | Structured EHR (Tables)       | Relational Data Reasoning    | 2,200 task-specific samples across 11 data and knowledge tasks.            | [GitHub](https://github.com/YXNTU/EHRStruct)                                                  |
| TCM-Eval                         | [TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine](https://arxiv.org/pdf/2511.07148)                                           | 2025.12.26                                                              | Text (TCM Knowledge)          | Professional MCQ             | 6,099 questions from expert-level Chinese medical examinations.            | [dataset](https://tcmeval.bamaidical.com/datasets)                                            |
| RxSafeBench                      | [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/pdf/2511.04328)                                  | BIBM2025                                                                | Text (Dialogue & MCQ)         | Medication Safety QA         | 2,443 consultation scenarios including 1,063 contraindication cases.       | [GitHub](https://github.com/DaneshjouLab/prompt-triage-lab)                                   |
| SemBench                         | [SemBench: A Benchmark for Semantic Query Processing Engines](https://arxiv.org/pdf/2511.01716)                                                                           | 2025.11.3                                                               | Text (Knowledge Graph)        | Semantic Query Evaluation    | 1,400+ SPARQL templates for evaluating medical query engines.              | [GitHub](https://github.com/SemBench/SemBench)                                                |
| XBench                           | [XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography](https://arxiv.org/pdf/2510.19599)                                               | 2025.10.22                                                              | Image (X-ray) & Text          | Grounding & Explanation      | 12,601 chest X-ray cases with localization and textual explanations.       | [GitHub](https://github.com/Roypic/Benchmarkingattention)                                     |
| IMB                              | [IMB: An Italian Medical Benchmark for Question Answering](https://arxiv.org/pdf/2510.18468)                                                                              | CLIC-it 2025                                                            | Text (Italian)                | Medical QA & MCQA            | 808,506 items featuring 782,644 clinical Italian conversations.            | [GitHub](https://github.com/PRAISELab-PicusLab/IMB)                                           |
| ViPET-ReportGen                  | [Toward a Vision-Language Foundation Model for Medical Data: Multimodal Dataset and Benchmarks for Vietnamese PET/CT Report Generation](https://arxiv.org/pdf/2509.24739) | NeurIPS 2025                                                            | Image (3D PET/CT) & Text      | Report Generation & VQA      | 1.5 million slices paired with 2,757 Vietnamese clinical reports.          | [GitHub](https://github.com/AIoT-Lab-AI4LIFE/ViPET-ReportGen)                                 |
| Neural-MedBench                  | [Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks](https://arxiv.org/pdf/2509.22258)                                          | 2025.12.13                                                              | Text & Image (MRI/CT)         | Differential Diagnosis       | 120 expert cases resulting in 200 depth-of-reasoning tasks.                | [Project page](https://neuromedbench.github.io/)                                              |
| MedQARo                          | [MedQARo: A Large-Scale Benchmark for Evaluating Large Language Models on Medical Question Answering in Romanian](https://arxiv.org/pdf/2508.16390)                       | 2025.12.31                                                              | Text (Romanian)               | Multilingual QA              | 102,646 Romanian QA pairs covering 1,011 clinical patients.                | [GitHub](https://github.com/ana-rogoz/MedQARo)                                                |
| AnesSuite                        | [AnesSuite: A Comprehensive Benchmark and Dataset Suite for Anesthesiology Reasoning in LLMs](https://arxiv.org/pdf/2504.02404)                                           | 2025.12.25                                                              | Text (Anesthesiology)         | Specialized Knowledge QA     | 4,427 anesthesiology MCQ items focused on complex decision-making.         | [GitHub](https://github.com/MiliLab/AnesSuite)                                                |
| TracSum                          | [TracSum: A New Benchmark for Aspect-Based Summarization with Sentence-Level Traceability in Medical Domain](https://arxiv.org/pdf/2508.13798)                            | the 2025 Conference on Empirical Methods in Natural Language Processing | Text (Abstracts/Notes)        | Aspect-Based Summarization   | 500 abstracts resulting in 3,500 summary-citation traceable pairs.         | [GitHub](https://github.com/chubohao/TracSum)                                                 |
| MedAgentBoard                    | [MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks](https://arxiv.org/pdf/2505.12371)                             | 2025.10.30                                                              | Text, Image & EHR             | Multi-Agent Collaboration    | 8 benchmark categories designed for multi-agent reasoning tasks.           | [Project page](https://medagentboard.netlify.app/)                                            |
| HEAL-MedVQA                      | [Localizing Before Answering: A Hallucination Evaluation Benchmark for Grounded Medical Multimodal LLMs](https://arxiv.org/pdf/2505.00744)                                | 2025IJCAI                                                               | Image & Text                  | Grounded Medical VQA         | 11,000+ samples requiring localization prior to medical answering.         | [Project page](https://loba-ijcai25.github.io/)                                               |
| BRIDGE                           | [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/pdf/2504.19467)                                        | 2025.10.28                                                              | Text (EHR & Notes)            | Multitask Evaluation         | 1.4 million samples covering 87 tasks in 9 different languages.            | [Project page](https://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard)            |
| LLMEval-Med                      | [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/pdf/2506.04078)                                               | 2025.8.31                                                               | Text                          | Clinical QA Validation       | ~1,000 real-world cases validated via physician-in-the-loop audits.        | [GitHub](https://github.com/llmeval/LLMEval-Med)                                              |
| CSEDB                            | [A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains](https://arxiv.org/pdf/2507.23486)                              | 2025.8.13                                                               | Text (Clinical Context)       | Safety-Effectiveness Eval    | 30 criteria across 26 specialties based on expert physician consensus.     | [GitHub](https://github.com/Medlinker-MG/CSEDB)                                               |
| SSG-VQA                          | [Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study](https://arxiv.org/pdf/2506.06232)                                     | 2025.7.8                                                                | Image & Text                  | Surgical VQA                 | 1,300+ scene graph samples focused on instrument-tissue interaction.       | [GitHub](https://github.com/CAMMA-public/SSG-VQA)                                             |
| M$^3$-Med                        | [M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal, and Multi-hop Reasoning in Medical Instructional Video Understanding](https://arxiv.org/pdf/2507.04289)           | 2025.7.6                                                                | Video & Text                  | Multi-hop Reasoning          | 3,748 instructional videos with 12,747 reasoning-intensive QA pairs.       | [Project page](https://med-m3-dataset.github.io/)                                             |
| PET2Rep | [PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography](https://arxiv.org/pdf/2508.04062) | 2025.08.06 | Text + Image (PET/CT) | Report Generation | 565 whole-body paired pet/ct data combinations with detailed radiology report | [Github](https://github.com/YichiZhang98/PET2Rep) |
| MedTVT-QA | [MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis](https://arxiv.org/pdf/2506.18512) | 2025.06.23 | Text + Time Series (ECG) + Image (CXR) + Tabular (Lab Test) | Multimodal Medical Reasoning, Multi-disease Diagnosis, Report Generation | 8,706 multimodal data combinations used to generate QA pairs | [Github](https://github.com/keke-nice/MedTVT-R1) |
| HIE-Reasoning | [Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning](https://openreview.net/pdf?id=tnyxtaSve5) | 2025.06.18(ICML2025) | Text + Image (MRI) + Clinical Data | Professional-level Medical Reasoning, Neurocognitive Outcome Prediction, Lesion Analysis | 133 unique MRIs, 749 professional QA pairs, 133 interpretation summaries | [Github](https://github.com/i3-research/HIE-Reasoning) |
| ReasonMed| [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513) | 2025.06.11 | Text (Multi-agent CoT, Summary, QA) | Medical Reasoning, QA, CoT Fine-tuning | 370K high-quality samples distilled from 1.75M CoT paths, based on 195K questions from 4 benchmarks | [HF](https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed) |
| Lingshu (Train) | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | Text + Image (Multimodal Instruction, VQA, Report) | Multimodal Medical QA, Reasoning, Consultation, Report Generation | ~9.3M training samples from 60+ datasets | [Project Page](https://alibaba-damo-academy.github.io/lingshu/) |
| MedEvalKit (Linshu Test) | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | Text + Image (Multimodal Benchmarks) | Benchmarking: VQA, Report Generation, Medical Text QA | 152,066 evaluation samples from 16 benchmarks | [Github](https://github.com/alibaba-damo-academy/MedEvalKit) |
| MIRIAD | [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091) | 2025.06.09 | Text (Instruction-Response) | Medical QA, Retrieval-Augmented Generation (RAG), Hallucination Detection | 5.8M / 4.4M QA pairs | [HF](https://huggingface.co/datasets/miriad/miriad-5.8M) |
| ClinBench-HPB | [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095) | 2025.06.04 | Text (Multiple-choice Questions, Clinical Cases) | Medical Question Answering, Hepato-Pancreato-Biliary Clinical Case Diagnosis | 3,535 MCQs & 337 clinical cases, covering 465+ Hepato-Pancreato-Biliary diseases | [Project Page](https://clinbench-hpb.github.io), [HF](https://huggingface.co/datasets/ASD9987/ClinBench-HPB) |
| SurgVLM-DB         |SurgVLM-DB: A Large-scale Multimodal Surgical Database Comprising Over 1.81 Million Frames with 7.79 Million Conversations                                                        | 2025.06                 | Video + Text               | Multimodal QA                  | 1.81M frames, 7.79M QAs       | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                   |
| EndoBench    | [EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis](https://arxiv.org/abs/2505.23601v1) | 2025.05.29   | Image + Text (Visual QA, Multimodal Tasks, Multi-level Visual Prompts) | Endoscopy Analysis, Medical Imaging, Multimodal Model Evaluation | Covers 4 endoscopy scenarios (Gastroscopy, Colonoscopy, Capsule Endoscopy, Surgical Endoscopy); includes 12 clinical tasks and 12 subtasks; 5 levels of visual prompt granularity; 6832 clinically validated VQA samples | [HF](https://huggingface.co/datasets/Saint-lsy/EndoBench) |
| MedXpertQA | MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding | ICML2025 | Text + Image (Multimodal MCQs) | Expert-level Medical QA, Clinical Reasoning, Multimodal Understanding | 4,460 questions (2,455 text / 2,005 image) | [HF](https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA) |
| MedCaseReasoning   | MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports                         | 2025.05.20              | Text                       | Diagnostic Reasoning           | 14,489 QA cases               | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                   |
| vlm-project-with-images-with-bbox-images-with-tree-of-thoughts | - | 2025.06                 | MRI Image + BBox + Multilingual QA (Including 7 languages: vi, en, fr, de, zh, ko, ja) | VQA, Lesion Detection, Clinical Reasoning (Tree-of-Thought) | 12.3k samples | [HF](https://huggingface.co/datasets/tungvu3196/vlm-project-with-images-with-bbox-images-with-tree-of-thoughts) | N/A |
| DrVD-Bench         | DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?                | 2025.05.30              | Medical Images + Text      | VQA, Reasoning, Report Gen.    | 7,789 imageâ€“QA pairs          | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine                                     | 2025.05                 | Text                       | Instruction Tuning             | 5M instances, 19K instructions| [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                            |
| MedS-Bench         | -                                                                                                | 2025.05                 | Text                       | Clinical Task Benchmark        | 11 task types                 | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                          |
| MM-Skin            | MM-Skin:Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks                    | 2025.05.09              | Image + Text               | Open-ended VQA (no reasoning)  | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                        |
| AlphaMed19K        | Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL                                                     | 2025.05.23              | Text                       | QA Reasoning                   | 19K QAs                       | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                           |
| Derm1M | [Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology](https://arxiv.org/pdf/2503.14911) | 2025.04.13(ICCV2025) | Text + Image (Dermatology) | Skin Disease Classification, Concept Identification, Cross-modal Retrieval | 1,029,761 image-text pairs | [Github](https://github.com/SiyuanYan1/Derm1M) |
| GEMeX | [GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis](https://arxiv.org/pdf/2411.16778) | 2025.03.23 (ICCV2025) | Text + Image (Chest X-ray) | Medical Visual Question Answering (VQA) for Chest X-ray Diagnosis | 151,025 images and 1,605,575 QA pairs | [HF](https://huggingface.co/datasets/BoKelvin/GEMeX-VQA), [Github](https://github.com/Awenbocc/GEMeX-Project) |
| Surg-396K | [EndoChat: Grounded Multimodal Large Language Model for Endoscopic Surgery](https://arxiv.org/abs/2501.11347) | 2025.03.15 | Image + Text (Multimodal Instruction, VQA, Grounding, Description) | Endoscopic Surgery, Surgical Scene Understanding, Visual QA, Grounded Dialogue | 396K instruction-image pairs from 41.4K images across 3 datasets (EndoVis, CoPESD, Cholec80) with 5 conversation types and 7 scene understanding tasks | [GitHub](https://github.com/gkw0010/EndoChat), [Data Link](https://mycuhk-my.sharepoint.com/personal/1155180074_link_cuhk_edu_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1155180074%5Flink%5Fcuhk%5Fedu%5Fhk%2FDocuments%2FRENLAB%20AI%20Data%2FSurg%2D396k&ga=1) |
| AbdomenAtlas 3.0 | [RadGPT: Constructing 3D Image-Text Tumor Datasets](https://arxiv.org/pdf/2501.04678) | 2025.01.08(ICCV2025) | Text + 3D Image (Abdominal CT) | 3D Abdominal CT Report Generation, Tumor Segmentation, Staging, and Analysis | 9,262 3D CT scans with paired reports, detailing 8,562 tumor instances | [Github](https://github.com/MrGiovanni/RadGPT), [HF](https://huggingface.co/datasets/SpamYdob/AbdomenAtlas3.0Report)|
| HuatuoGPT-o1 Dataset | [HuatuoGPT-o1ï¼ŒTowards Medical Complex Reasoning with LLMs](https://arxiv.org/pdf/2412.18925) | 2024.12.25 | Text (Complex CoT, Medical Verifiable Problems, Multi-Step Reasoning) | Medical Complex Reasoning, CoT Fine-tuning, Reinforcement Learning | Contains 40K high-quality medical complex reasoning problems filtered by a medical verifier, based on MedQA-USMLE and MedMCQA medical exam training sets | [GitHub](https://github.com/FreedomIntelligence/HuatuoGPT-o1), [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) |
| PubMedVision        | [HuatuoGPT-Visionn, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale](https://arxiv.org/abs/2406.19280)                         | 2024.09.30 | Image + Text (Multimodal) | Medical VQA (Alignment VQA, Instruction-Tuning VQA), Captioning, Summarization | 1.3M VQA samples from 914,960 filtered PubMed medical images & text (647K + 647K)     | [Hugging Face](https://huggingface.co/datasets/FreedomIntelligence/PubMedVision)|
| PMC-VQA            | PMC-VQA: Visual Instruction Tuning for Medical VQA                                                       | 2024.08.08              | Image + Text               | VQA                            | 226,946 QA pairs              | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                            |
| VQARad             | - | 2023.08.07              | Radiography                | VQA                            | 315 images, 3,515 QAs         | [OSF](https://osf.io/89kps/)                                                                         |
|  Asclepius | [Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2402.11217) | ACL 2025 | Image + Text | VQA | 3232 VQA pairs encompassing 15 medical specialties, stratifying into 3 main categories and 8 sub-categories of clinical tasks | [GitHub](https://github.com/Asclepius-Med/Asclepius) |
|  MedTrinity-25M | [MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine](https://yunfeixie233.github.io/MedTrinity-25M/) | ICLR 2025 | Image + Text | VQA | 25M VQA pairs, covering over 25 million images across 10 modalities | [HF](https://huggingface.co/datasets/UCSC-VLAA/MedTrinity-25M) |
|  MediConfusion | [MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models](https://arxiv.org/abs/2409.15477) | ICLR 2025 | Image + Text | VQA | 176 confusing pairs, a set of two images that share the same question and corresponding answer options, but the correct answer is different for the images. | [HF](https://huggingface.co/datasets/shahab7899/MediConfusion) |
| GMAI-MMBench       | GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI                     | NeurIPS 2024            | Multi-modal (38 types)     | VQA                            | 26K QA pairs                  | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                       |
| PathMMU            | PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology       | 2024.03.20              | Pathology Image + Text     | Multi-choice, Reasoning        | 33,428 QAs, 24,067 images     | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                             |
| OmniMedVQA         | OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM                          | CVPR 2024               | Multi-modal (12 types)     | VQA                            | 118,010 images, 127,995 QA    | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                       |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models                 | NeurIPS 2024            | Medical Images + QA        | Open/Closed QA                 | 41K QA pairs                  | [GitHub](https://github.com/richard-peng-xia/CARES)                                                |
| MultiMedEval       | MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models                        | 2024.02.16              | Image + Text               | Multi-task Evaluation          | 6 tasks, 23 datasets          | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                             |
| medical-o1-reasoning-SFT       | HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs | ACL 2025          | Medical VQAã€Reasoning            | Medical VQA                         | 19.7k QA pairs                      | [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT)                                     |

---

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆæœ¬

### ğŸ” é¡¹ç›®ç®€ä»‹

éšç€åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-VLMï¼‰åŠå…¶æ¨ç†èƒ½åŠ›ç ”ç©¶çš„æŒç»­æ¨è¿›ï¼Œå°¤å…¶åœ¨ 2025 å¹´ 3 æœˆè‡³ 5 æœˆæœŸé—´ï¼Œé™†ç»­å‘å¸ƒäº†ä¼—å¤šé«˜è´¨é‡ã€èšç„¦äºåŒ»å­¦æ¨ç†èƒ½åŠ›çš„æ–°å‹å…¬å¼€æ•°æ®é›†ï¼Œä¸ºå¤šæ¨¡æ€åŒ»ç–—äººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½æ±‡æ€»è¿™ä¸€äº›æ•°æ®ï¼ŒæœŸå¾…èƒ½ä¸ºè¯¥ç¤¾åŒºæä¾›æ›´ä¾¿æ·çš„æ•°æ®è®¿é—®æ–¹å¼ã€‚æˆ‘ä»¬å‘å¸ƒäº†Med-VLM-Benchï¼š

**Med-VLM-Bench** è‡´åŠ›äºæ±‡æ€»å¹¶æ•´ç†è¿™äº›æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°çš„å…³é”®èµ„æºï¼š

- âœ… èšç„¦ 2025 å¹´ 3æœˆâ€“2026å¹´å‘å¸ƒçš„æ–°æ•°æ®é›†  
- ğŸ§  é‡ç‚¹å¼ºè°ƒæ¨ç†èƒ½åŠ›ã€å¤šæ¨¡æ€ç†è§£å’Œé—®ç­”èƒ½åŠ›çš„æ•°æ®é›† 
- ğŸ§ª åŒæ—¶è¦†ç›– 2023â€“2024 å¹´çš„ç»å…¸Med LLM/VLM benchmark datasets
- ğŸ”— æä¾›ç›´æ¥å¯ç”¨çš„ä¸‹è½½é“¾æ¥å’Œå¼€æºåœ°å€

ğŸ’¡ æˆ‘ä»¬çš„çŸ¥è¯†æ¥æºæœ‰é™ï¼Œæ¬¢è¿å¤§å®¶é€šè¿‡ Issue æˆ– PR æ¨èæ›´å¤šæ•°æ®é›†ï¼Œæˆ‘ä»¬ä¼šç¬¬ä¸€æ—¶é—´æ›´æ–°ï¼

ğŸ“ŒNote: æ­¤å¤–æˆ‘ä»¬çš„æ•°æ®é›†æ ‡æ³¨æ—¶é—´ä»¥ç›¸åº”æ–‡ç« å‘è¡¨æ—¶é—´ä¸ºå‡†

---

### ğŸ“Š æ•°æ®é›†æ±‡æ€»è¡¨

| æ•°æ®é›†åç§°         | è®ºæ–‡æ ‡é¢˜                                                                 | å¹´ä»½ / ä¼šè®®              | æ•°æ®æ¨¡æ€                  | ä»»åŠ¡ç±»å‹                        | æ•°æ®è§„æ¨¡                       | ä¸‹è½½é“¾æ¥                                                                                         |
|--------------------|--------------------------------------------------------------------------|---------------------------|---------------------------|----------------------------------|-------------------------------|--------------------------------------------------------------------------------------------------|
| Multi-RADS                       | [Multi-RADS Synthetic Radiology Report Dataset...](https://arxiv.org/pdf/2601.03232)            | 2026.1.6     | æ–‡æœ¬ (åˆæˆæŠ¥å‘Š)             | RADS åˆ†ç±»          | 1,600 ä»½åˆæˆæŠ¥å‘Šï¼Œæ¶µç›– 17 ç§å½±åƒå‘ç°ã€‚           | [Github](https://github.com/RadioX-Labs/RADSet)                                      |
| Bones and Joints (B&J) Benchmark | [The Illusion of Clinical Reasoning...](https://arxiv.org/pdf/2512.22275)                       | 2025.12.25   | æ–‡æœ¬ä¸å›¾åƒ (Xå…‰, CT, MRI)   | è§†è§‰é—®ç­” (VQA) ä¸æ²»ç–—è®¡åˆ’ | 1,245 ä¸ªé—®ç­”å¯¹ï¼Œæ¶µç›– 7 é¡¹ä¸´åºŠèƒ½åŠ›ä»»åŠ¡ã€‚           | [ Hugging Face](https://huggingface.co/datasets/PUTH2025/Bones_and_Joints_Benchmark) |
| MediEval                         | [MediEval: A Unified Medical Benchmark...](https://arxiv.org/pdf/2512.20822)                    | 2025.12.23   | æ–‡æœ¬ (ç”µå­å¥åº·è®°å½•ä¸ç¬”è®°)        | è‡ªç„¶è¯­è¨€æ¨ç† (NLI)     | 37,144 æ¡åŒ»ç–—é™ˆè¿°ï¼Œæºè‡ª 2,015 æ¬¡å…¥é™¢è®°å½•ã€‚       | [GitHub](https://anonymous.4open.science/r/MediEval-5FA5/)                           |
| TCM-BEST4SDT                     | [A benchmark dataset for evaluating Syndrome...](https://arxiv.org/pdf/2512.02816)              | 2025.12.2    | æ–‡æœ¬ (ä¸­åŒ»ç—…å†æŠ¥å‘Š)           | è¾¨è¯è®ºæ²»             | æ€»è®¡ 600 é“é¢˜ç›®ï¼ŒåŒ…å« 300 ä¾‹ä¸´åºŠè¾¨è¯æ¡ˆä¾‹ã€‚         | [GitHub](https://github.com/DYJG-research/TCM-BEST4SDT)                              |
| SurgMLLMBench                    | [SurgMLLMBench: A Multimodal Large Language Model...](https://arxiv.org/pdf/2511.21339)         | 2025.11.26   | è§†é¢‘ä¸æ–‡æœ¬                 | æ‰‹æœ¯åœºæ™¯ç†è§£           | 10,652 å¸§æ ‡æ³¨å›¾åƒï¼Œæ•´åˆè‡ª 5 ä¸ªæ‰‹æœ¯æ•°æ®é›†ã€‚         | [project page](https://surgmllmbench.github.io/)                                     |
| MedVision                        | [MedVision: Dataset and Benchmark for Quantitative...](https://arxiv.org/pdf/2511.18676)        | 2025.11.24   | å›¾åƒ (CT, MRI, Xå…‰, PET) | æ£€æµ‹ä¸æµ‹é‡            | 3,080 ä¸‡ä¸ªå›¾åƒ-æ ‡æ³¨å¯¹ï¼Œæ¶µç›– 22 ä¸ªå…¬å…±æ•°æ®é›†ã€‚       | [project page](https://medvision-vlm.github.io/)                                     |
| EHRStruct                        | [EHRStruct: A Comprehensive Benchmark Framework...](https://arxiv.org/pdf/2511.08206)           | 2025.12.1    | ç»“æ„åŒ–ç”µå­å¥åº·è®°å½• (è¡¨æ ¼)        | å…³ç³»æ•°æ®æ¨ç†           | 2,200 ä¸ªé’ˆå¯¹ 11 é¡¹ä»»åŠ¡çš„ç‰¹å®šè¯„ä¼°æ ·æœ¬ã€‚           | [GitHub](https://github.com/YXNTU/EHRStruct)                                         |
| TCM-Eval                         | [TCM-Eval: An Expert-Level Dynamic and Extensible...](https://arxiv.org/pdf/2511.07148)         | 2025.12.26   | æ–‡æœ¬ (ä¸­åŒ»çŸ¥è¯†)             | ä¸“ä¸šé€‰æ‹©é¢˜ (MCQ)      | 6,099 é“æ¥è‡ªä¸“å®¶çº§ä¸­åŒ»è€ƒè¯•çš„é¢˜ç›®ã€‚               | [dataset](https://tcmeval.bamaidical.com/datasets)                                   |
| RxSafeBench                      | [RxSafeBench: Identifying Medication Safety Issues...](https://arxiv.org/pdf/2511.04328)        | BIBM2025     | æ–‡æœ¬ (å¯¹è¯ä¸é€‰æ‹©é¢˜)           | ç”¨è¯å®‰å…¨é—®ç­”           | 2,443 ä¸ªå’¨è¯¢åœºæ™¯ï¼ŒåŒ…å« 1,063 ä¾‹ç¦å¿Œç—‡æ¡ˆä¾‹ã€‚       | [GitHub](https://github.com/DaneshjouLab/prompt-triage-lab)                          |
| SemBench                         | [SemBench: A Benchmark for Semantic Query Processing...](https://arxiv.org/pdf/2511.01716)      | 2025.11.3    | æ–‡æœ¬ (çŸ¥è¯†å›¾è°±)             | è¯­ä¹‰æŸ¥è¯¢è¯„ä¼°           | 1,400 å¤šä¸ªç”¨äºè¯„ä¼°åŒ»ç–—æŸ¥è¯¢å¼•æ“çš„æ¨¡æ¿ã€‚             | [GitHub](https://github.com/SemBench/SemBench)                                       |
| XBench                           | [XBench: A Comprehensive Benchmark for Visual...](https://arxiv.org/pdf/2510.19599)             | 2025.10.22   | å›¾åƒ (Xå…‰) ä¸æ–‡æœ¬           | å®šä½ä¸è§£é‡Š            | 12,601 ä¾‹é™„å¸¦å®šä½ä¸æ–‡æœ¬è§£é‡Šçš„èƒ¸ç‰‡æ¡ˆä¾‹ã€‚            | [GitHub](https://github.com/Roypic/Benchmarkingattention)                            |
| IMB                              | [IMB: An Italian Medical Benchmark for Question Answering](https://arxiv.org/pdf/2510.18468)    | CLIC-it 2025 | æ–‡æœ¬ (æ„å¤§åˆ©è¯­)             | åŒ»ç–—é—®ç­”ä¸é€‰æ‹©é¢˜         | 808,506 æ¡æ•°æ®ï¼ŒåŒ…å« 78 ä¸‡æ¡ä¸´åºŠå¯¹è¯ã€‚          | [GitHub](https://github.com/PRAISELab-PicusLab/IMB)                                  |
| ViPET-ReportGen                  | [Toward a Vision-Language Foundation Model for Medical...](https://arxiv.org/pdf/2509.24739)    | NeurIPS 2025 | å›¾åƒ (3D PET/CT) ä¸æ–‡æœ¬    | æŠ¥å‘Šç”Ÿæˆä¸è§†è§‰é—®ç­”        | 150 ä¸‡å¼ åˆ‡ç‰‡ï¼Œé…å¯¹ 2,757 ä»½è¶Šå—è¯­ä¸´åºŠæŠ¥å‘Šã€‚        | [GitHub](https://github.com/AIoT-Lab-AI4LIFE/ViPET-ReportGen)                        |
| Neural-MedBench                  | [Beyond Classification Accuracy: Neural-MedBench...](https://arxiv.org/pdf/2509.22258)          | 2025.12.13   | æ–‡æœ¬ä¸å›¾åƒ (MRI/CT)        | é‰´åˆ«è¯Šæ–­             | 120 ä¸ªä¸“å®¶ç—…ä¾‹ï¼Œè¡ç”Ÿå‡º 200 ä¸ªæ·±åº¦æ¨ç†ä»»åŠ¡ã€‚         | [Project page](https://neuromedbench.github.io/)                                     |
| AnesSuite                        | [AnesSuite: A Comprehensive Benchmark and Dataset...](https://arxiv.org/pdf/2504.02404)         | 2025.12.25   | æ–‡æœ¬ (éº»é†‰å­¦)              | ä¸“ä¸šçŸ¥è¯†é—®ç­”           | 4,427 ä¸ªä¸“æ³¨äºå¤æ‚å†³ç­–çš„éº»é†‰å­¦é€‰æ‹©é¢˜ã€‚             | [GitHub](https://github.com/MiliLab/AnesSuite)                                       |
| MedQARo                          | [MedQARo: A Large-Scale Benchmark for Evaluating...](https://arxiv.org/pdf/2508.16390)          | 2025.12.31   | æ–‡æœ¬ (ç½—é©¬å°¼äºšè¯­)            | å¤šè¯­è¨€é—®ç­”            | 102,646 ä¸ªç½—é©¬å°¼äºšè¯­é—®ç­”å¯¹ï¼Œæ¶µç›– 1,011 åæ‚£è€…ã€‚    | [GitHub](https://github.com/ana-rogoz/MedQARo)                                       |
| TracSum                          | [TracSum: A New Benchmark for Aspect-Based Summarization...](https://arxiv.org/pdf/2508.13798)  | EMNLP 2025   | æ–‡æœ¬ (æ‘˜è¦/ç¬”è®°)            | åŸºäºç»´åº¦çš„åŒ»ç–—æ‘˜è¦        | 500 ç¯‡æ‘˜è¦ï¼Œç”Ÿæˆ 3,500 ä¸ªå¯è¿½æº¯çš„æ‘˜è¦-å¼•ç”¨å¯¹ã€‚      | [GitHub](https://github.com/chubohao/TracSum)                                        |
| MedAgentBoard                    | [MedAgentBoard: Benchmarking Multi-Agent Collaboration...](https://arxiv.org/pdf/2505.12371)    | 2025.10.30   | æ–‡æœ¬, å›¾åƒä¸è®°å½•             | å¤šæ™ºèƒ½ä½“åä½œ           | 8 ä¸ªåŸºå‡†ç±»åˆ«ï¼Œä¸“ä¸ºå¤šæ™ºèƒ½ä½“æ¨ç†ä»»åŠ¡è®¾è®¡ã€‚              | [Project page](https://medagentboard.netlify.app/)                                   |
| HEAL-MedVQA                      | [Localizing Before Answering: A Hallucination Evaluation...](https://arxiv.org/pdf/2505.00744)  | IJCAI 2025   | å›¾åƒä¸æ–‡æœ¬                 | åŸºäºå®šä½çš„åŒ»ç–— VQA      | 11,000 å¤šä¸ªè¦æ±‚åœ¨å›ç­”å‰å…ˆå®šä½ç—…ç†åŒºåŸŸçš„æ ·æœ¬ã€‚         | [Project page](https://loba-ijcai25.github.io/)                                      |
| BRIDGE                           | [BRIDGE: Benchmarking Large Language Models for...](https://arxiv.org/pdf/2504.19467)           | 2025.10.28   | æ–‡æœ¬ (ç”µå­å¥åº·è®°å½•ä¸ç¬”è®°)        | å¤šä»»åŠ¡è¯„ä¼°            | 140 ä¸‡ä¸ªæ ·æœ¬ï¼Œæ¶µç›– 9 ç§è¯­è¨€çš„ 87 é¡¹ä»»åŠ¡ã€‚         | [Project page](https://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard)   |
| LLMEval-Med                      | [LLMEval-Med: A Real-world Clinical Benchmark for Medical...](https://arxiv.org/pdf/2506.04078) | 2025.8.31    | æ–‡æœ¬                    | ä¸´åºŠé—®ç­”éªŒè¯           | çº¦ 1,000 ä¾‹é€šè¿‡åŒ»å¸ˆå‚ä¸å®¡è®¡éªŒè¯çš„çœŸå®æ¡ˆä¾‹ã€‚          | [GitHub](https://github.com/llmeval/LLMEval-Med)                                     |
| CSEDB                            | [A Novel Evaluation Benchmark for Medical LLMs...](https://arxiv.org/pdf/2507.23486)            | 2025.8.13    | æ–‡æœ¬ (ä¸´åºŠè¯­å¢ƒ)             | å®‰å…¨æ€§-æœ‰æ•ˆæ€§è¯„ä¼°        | åŸºäºä¸“å®¶å…±è¯†çš„ 30 é¡¹æ ‡å‡†ï¼Œæ¶µç›– 26 ä¸ªä¸“ç§‘ã€‚          | [GitHub](https://github.com/Medlinker-MG/CSEDB)                                      |
| SSG-VQA                          | [Challenging Vision-Language Models with Surgical Data...](https://arxiv.org/pdf/2506.06232)    | 2025.7.8     | å›¾åƒä¸æ–‡æœ¬                 | æ‰‹æœ¯è§†è§‰é—®ç­” (VQA)     | 1,300 å¤šä¸ªä¸“æ³¨äºå™¨æ¢°-ç»„ç»‡äº¤äº’çš„åœºæ™¯å›¾æ ·æœ¬ã€‚          | [GitHub](https://github.com/CAMMA-public/SSG-VQA)                                    |
| M$^3$-Med                        | [M$^3$-Med: A Benchmark for Multi-lingual, Multi-modal...](https://arxiv.org/pdf/2507.04289)    | 2025.7.6     | è§†é¢‘ä¸æ–‡æœ¬                 | å¤šè·³æ¨ç†             | 3,748 æ®µæ•™å­¦è§†é¢‘ï¼ŒåŒ…å« 12,747 ä¸ªé‡æ¨ç†é—®ç­”å¯¹ã€‚     | [Project page](https://med-m3-dataset.github.io/)                                    |
| PET2Rep | [PET2Rep: Towards Vision-Language Model-Drived Automated Radiology Report Generation for Positron Emission Tomography](https://arxiv.org/pdf/2508.04062) | 2025.08.06 | Text + Image (PET/CT) | æŠ¥å‘Šç”Ÿæˆ | 565å…¨èº«é…å¯¹PET/CTæ•°æ®ç»„åˆä¸è¯¦ç»†çš„æ”¾å°„å­¦æŠ¥å‘Š | [Github](https://github.com/YichiZhang98/PET2Rep) |
| MedTVT-QA | [MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis](https://arxiv.org/pdf/2506.18512) | 2025.06.23 | æ–‡æœ¬ + æ—¶é—´åºåˆ— (å¿ƒç”µå›¾) + å›¾åƒ (èƒ¸éƒ¨Xå…‰) + è¡¨æ ¼ (è¡€æ¶²æ£€æµ‹) | å¤šæ¨¡æ€åŒ»ç–—æ¨ç†ã€å¤šç—…ç§è¯Šæ–­ã€æŠ¥å‘Šç”Ÿæˆ | ç”¨äºç”ŸæˆQAå¯¹çš„8,706ç»„å¤šæ¨¡æ€æ•°æ®ç»„åˆ | [Github](https://github.com/keke-nice/MedTVT-R1) |
| HIE-Reasoning | [Visual and Domain Knowledge for Professional-level Graph-of-Thought Medical Reasoning](https://openreview.net/pdf?id=tnyxtaSve5) | 2025.06.18(ICML2025) | æ–‡æœ¬ + å›¾åƒ (MRI) + ä¸´åºŠæ•°æ® | ä¸“ä¸šçº§åŒ»ç–—æ¨ç†ã€ç¥ç»è®¤çŸ¥ç»“å±€é¢„æµ‹ã€ç—…ç¶åˆ†æ | 133ä¸ªç‹¬ç«‹MRIã€749ä¸ªä¸“ä¸šé—®ç­”å¯¹ã€133ä»½è§£è¯»æ‘˜è¦ | [Github](https://github.com/i3-research/HIE-Reasoning) |
| ReasonMed | [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513) | 2025.06.11 | æ–‡æœ¬ï¼ˆå¤šä»£ç†æ¨ç†ã€å¤šæ­¥æ€»ç»“ã€åŒ»å­¦é—®ç­”ï¼‰ | åŒ»å­¦æ¨ç†ã€é—®ç­”ã€é“¾å¼æ€ç»´å¾®è°ƒ | ä» 175 ä¸‡æ¡ CoT è·¯å¾„ä¸­ç²¾ç‚¼å‡ºçš„ 37 ä¸‡é«˜è´¨é‡æ ·æœ¬ï¼Œè¦†ç›–æ¥è‡ª 4 ä¸ªåŸºå‡†çš„ 19.5 ä¸‡é—®é¢˜ | [HF](https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed) |
| Lingshuï¼ˆTrainï¼‰ | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | æ–‡æœ¬ + å›¾åƒï¼ˆå¤šæ¨¡æ€æŒ‡ä»¤ã€VQAã€æŠ¥å‘Šï¼‰ | å¤šæ¨¡æ€åŒ»å­¦é—®ç­”ã€æ¨ç†ã€é—®è¯Šã€æŠ¥å‘Šç”Ÿæˆ | çº¦ 930 ä¸‡è®­ç»ƒæ ·æœ¬ï¼Œæ¥è‡ª 60+ æ•°æ®é›† | [Project Page](https://alibaba-damo-academy.github.io/lingshu/) |
| MedEvalKitï¼ˆLinshu testï¼‰ | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | æ–‡æœ¬ + å›¾åƒï¼ˆå¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ï¼‰ | åŸºå‡†è¯„æµ‹ï¼šVQAã€æŠ¥å‘Šç”Ÿæˆã€åŒ»å­¦æ–‡æœ¬é—®ç­” | å…± 152,066 ä¸ªè¯„ä¼°æ ·æœ¬ï¼Œæ¥è‡ª 16 ä¸ªåŸºå‡†æ•°æ®é›† | [Github](https://github.com/alibaba-damo-academy/MedEvalKit) |
| MIRIAD | [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091) | 2025.06.09 | æ–‡æœ¬ï¼ˆæŒ‡ä»¤-å›ç­”å¯¹ï¼‰ | åŒ»å­¦é—®ç­”ã€RAG æ£€ç´¢å¢å¼ºã€å¹»è§‰æ£€æµ‹ |  582 ä¸‡ / 448 ä¸‡ | [HF](https://huggingface.co/datasets/miriad/miriad-5.8M) |
| ClinBench-HPB | [ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases](https://arxiv.org/abs/2506.00095) | 2025.06.04 | æ–‡æœ¬ (é€‰æ‹©é¢˜, ä¸´åºŠç—…ä¾‹) | åŒ»å­¦é—®ç­”, ä¸´åºŠç—…ä¾‹è¯Šæ–­ | 3,535é“é€‰æ‹©é¢˜å’Œ337ä¸ªä¸´åºŠç—…ä¾‹, è¦†ç›–465+ç§è‚èƒ†èƒ°ç–¾ç—… | [Project Page](https://clinbench-hpb.github.io), [HF](https://huggingface.co/datasets/ASD9987/ClinBench-HPB) |
| SurgVLM-DB         |SurgVLM-DB: A Large-Scale Multimodal Surgical Database                               | 2025.06                   | è§†é¢‘ + æ–‡æœ¬               | å¤šæ¨¡æ€é—®ç­”                      | 1.81Må¸§, 7.79Må¯¹è¯             | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                 |
| EndoBench  | [EndoBench: A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis](https://arxiv.org/abs/2505.23601v1) | 2025.05.29 | å›¾åƒ+æ–‡æœ¬ï¼ˆè§†è§‰é—®ç­”ã€å¤šæ¨¡æ€ä»»åŠ¡ã€å¤šå±‚æ¬¡è§†è§‰æç¤ºï¼‰ | å†…é•œåˆ†æã€åŒ»å­¦å½±åƒã€å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼° | è¦†ç›–èƒƒé•œã€ç»“è‚ é•œã€èƒ¶å›Šå†…é•œå’Œæ‰‹æœ¯å†…é•œ 4 å¤§åœºæ™¯ï¼›åŒ…å« 12 ä¸ªä¸´åºŠä»»åŠ¡åŠ 12 ä¸ªæ¬¡ä»»åŠ¡ï¼›5 ç§è§†è§‰æç¤ºç²’åº¦ï¼›6832 ä¸ªç»è¿‡ä¸´åºŠéªŒè¯çš„ VQA æ ·æœ¬ | [HF](https://huggingface.co/datasets/Saint-lsy/EndoBench) |
| MedXpertQA | MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding | ICML2025 | æ–‡æœ¬ + å›¾åƒï¼ˆå¤šæ¨¡æ€é€‰æ‹©é¢˜ï¼‰ | ä¸“å®¶çº§åŒ»å­¦é—®ç­”ã€ä¸´åºŠæ¨ç†ã€å¤šæ¨¡æ€ç†è§£ | å…± 4,460 é¢˜ï¼ˆæ–‡æœ¬ 2,455 / å›¾åƒ 2,005ï¼‰ | [HF](https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA) |
| MedCaseReasoning   |MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports  | 2025.05.20                | æ–‡æœ¬                      | è¯Šæ–­æ¨ç†                        | 14,489é—®ç­”å¯¹                   | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                 |
| vlm-project-with-images-with-bbox-images-with-tree-of-thoughts | - | 2025.06 | MRI å›¾åƒ + BBox + å¤šè¯­ç§é—®ç­”ï¼ˆvi, en, fr, de, zh, ko, jaï¼‰ | åŒ»å­¦VQAã€ç—…ç¶æ£€æµ‹ã€ä¸´åºŠæ¨ç†ï¼ˆTree-of-Thoughtï¼‰ | 12,325 æ¡æ ·æœ¬ | [HF](https://huggingface.co/datasets/tungvu3196/vlm-project-with-images-with-bbox-images-with-tree-of-thoughts) | æœªæ³¨æ˜ |
| DrVD-Bench         | DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis? | 2025.05.30          | åŒ»å­¦å›¾åƒ + æ–‡æœ¬           | åŒ»å­¦VQAã€æ¨ç†ã€æŠ¥å‘Šç”Ÿæˆ         | 7,789å›¾æ–‡QAå¯¹                  | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine              | 2025.05                   | æ–‡æœ¬                      | æŒ‡ä»¤å¾®è°ƒ                        | 5Mæ ·æœ¬, 19KæŒ‡ä»¤                | [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                         |
| MedS-Bench         | -                                                                         | 2025.05                   | æ–‡æœ¬                      | ä¸´åºŠä»»åŠ¡è¯„ä¼°                    | 11å¤§ç±»ä»»åŠ¡                     | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                       |
| MM-Skin            | MM-Skin:Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks | 2025.05.09          | å›¾åƒ + æ–‡æœ¬               | å¼€æ”¾å¼VQAï¼ˆæ— æ¨ç†ï¼‰             | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                      |
| AlphaMed19K        | Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL                              | 2025.05.23                | æ–‡æœ¬                      | æ¨ç†é—®ç­”                        | 19Ké—®ç­”å¯¹                      | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                         |
| Derm1M | [Derm1M: A Million-scale Vision-Language Dataset Aligned with Clinical Ontology Knowledge for Dermatology](https://arxiv.org/pdf/2503.14911) | 2025.04.13(ICCV2025) | æ–‡æœ¬ + å›¾åƒï¼ˆçš®è‚¤ç—…å­¦ï¼‰ | çš®è‚¤ç—…åˆ†ç±»ã€æ¦‚å¿µè¯†åˆ«ã€è·¨æ¨¡æ€æ£€ç´¢ | 1,029,761ä¸ªå›¾æ–‡å¯¹ | [Github](https://github.com/SiyuanYan1/Derm1M) |
| GEMeX | [GEMeX: A Large-Scale, Groundable, and Explainable Medical VQA Benchmark for Chest X-ray Diagnosis](https://arxiv.org/pdf/2411.16778) | 2025.03.23 (ICCV2025) | æ–‡æœ¬ + å›¾åƒï¼ˆèƒ¸éƒ¨Xå…‰ç‰‡ï¼‰ | ç”¨äºèƒ¸éƒ¨Xå…‰è¯Šæ–­çš„åŒ»ç–—è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ | 151,025å¼ å›¾ç‰‡å’Œ1,605,575ä¸ªé—®ç­” | [HF](https://huggingface.co/datasets/BoKelvin/GEMeX-VQA), [Github](https://github.com/Awenbocc/GEMeX-Project) |
| Surg-396K | [EndoChat: Grounded Multimodal Large Language Model for Endoscopic Surgery](https://arxiv.org/abs/2501.11347) | 2025.03.15 | å›¾åƒ + æ–‡æœ¬ï¼ˆå¤šæ¨¡æ€æŒ‡ä»¤ã€é—®ç­”ã€ç›®æ ‡å®šä½ã€è¯¦ç»†æè¿°ï¼‰ | å†…çª¥é•œå¤–ç§‘ã€æ‰‹æœ¯åœºæ™¯ç†è§£ã€è§†è§‰é—®ç­”ã€å®šä½å¯¹è¯ | æ¥è‡ª EndoVisã€CoPESD å’Œ Cholec80 çš„ 41,400 å¼ å›¾åƒï¼Œç”Ÿæˆ 396,000 å›¾æ–‡å¯¹ï¼Œè¦†ç›– 5 ç§å¯¹è¯ç±»å‹ä¸ 7 ç±»æ‰‹æœ¯ç†è§£ä»»åŠ¡ | [GitHub](https://github.com/gkw0010/EndoChat) [Data link](https://mycuhk-my.sharepoint.com/personal/1155180074_link_cuhk_edu_hk/_layouts/15/onedrive.aspx?id=%2Fpersonal%2F1155180074%5Flink%5Fcuhk%5Fedu%5Fhk%2FDocuments%2FRENLAB%20AI%20Data%2FSurg%2D396k&ga=1)|
| AbdomenAtlas 3.0 | [RadGPT: Constructing 3D Image-Text Tumor Datasets](https://arxiv.org/pdf/2501.04678) | 2025.01.08 (ICCV2025) | æ–‡æœ¬ + 3Då›¾åƒ (è…¹éƒ¨CT) | 3Dè…¹éƒ¨CTæŠ¥å‘Šç”Ÿæˆã€è‚¿ç˜¤åˆ†å‰²ã€åˆ†æœŸä¸åˆ†æ | 9,262ç»„3D CTæ‰«æåŠé…å¯¹æŠ¥å‘Šï¼ŒåŒ…å«8,562ä¸ªè‚¿ç˜¤å®ä¾‹ | [Github](https://github.com/MrGiovanni/RadGPT), [HF](https://huggingface.co/datasets/SpamYdob/AbdomenAtlas3.0Report)|
| HuatuoGPT-o1 Dataset | [HuatuoGPT-o1ï¼ŒTowards Medical Complex Reasoning with LLMs](https://arxiv.org/pdf/2412.18925) | 2024.12.25 | æ–‡æœ¬ï¼ˆå¤æ‚é“¾å¼æ€ç»´ã€åŒ»å­¦éªŒè¯é¢˜ã€å¤šæ­¥æ¨ç†ï¼‰ | åŒ»å­¦å¤æ‚æ¨ç†ã€é“¾å¼æ€ç»´å¾®è°ƒã€å¼ºåŒ–å­¦ä¹  | åŒ…å« 40K ç»åŒ»å­¦éªŒè¯å™¨ç­›é€‰çš„é«˜è´¨é‡åŒ»å­¦å¤æ‚æ¨ç†é—®é¢˜ï¼ŒåŸºäº MedQA-USMLE å’Œ MedMCQA åŒ»å­¦è€ƒè¯•è®­ç»ƒé›† | [GitHub](https://github.com/FreedomIntelligence/HuatuoGPT-o1), [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT) |
| PubMedVision         | [HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale](https://arxiv.org/abs/2406.19280)                         | 2024.09.30 | å›¾åƒ + æ–‡æœ¬ï¼ˆå¤šæ¨¡æ€ï¼‰     | åŒ»å­¦è§†è§‰é—®ç­”ï¼ˆVQAï¼‰ã€å›¾æ–‡å¯¹é½ã€æŒ‡ä»¤å¾®è°ƒã€æè¿°ç”Ÿæˆç­‰                         | 130 ä¸‡ VQA æ ·æœ¬ï¼Œæ¥è‡ª PubMed ä¸­ç­›é€‰çš„ 91.5 ä¸‡åŒ»å­¦å›¾åƒä¸ä¸Šä¸‹æ–‡ï¼ˆ647K + 647Kï¼‰        | [HF](https://huggingface.co/datasets/FreedomIntelligence/PubMedVision)      |
| PMC-VQA            | PMC-VQA: Visual Instruction Tuning for Medical VQA                                 | 2024.08.08                | å›¾åƒ + æ–‡æœ¬               | åŒ»å­¦VQA                         | 226,946é—®ç­”å¯¹                  | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                          |
| VQARad             |-| 2023.08.07                | æ”¾å°„å›¾åƒ                  | VQA                             | 315å›¾åƒ, 3515é—®ç­”              | [OSF](https://osf.io/89kps/)                                                                       |
|  Asclepius | [Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models](https://arxiv.org/abs/2402.11217) | ACL 2025 | å›¾åƒ + æ–‡æœ¬ | åŒ»å­¦VQA | 3232æ¡é—®ç­”å¯¹ï¼Œæ¶µç›– 15 ä¸ªåŒ»å­¦ä¸“ä¸šï¼Œåˆ†ä¸º 3 ä¸ªä¸»è¦ç±»åˆ«å’Œ 8 ä¸ªå­ç±»åˆ«çš„ä¸´åºŠä»»åŠ¡ | [GitHub](https://github.com/Asclepius-Med/Asclepius) |
|  MedTrinity-25M | [MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine](https://yunfeixie233.github.io/MedTrinity-25M/) | ICLR 2025 | å›¾åƒ + æ–‡æœ¬ | åŒ»å­¦VQA | å¤§è§„æ¨¡åŒ»å­¦å¤šæ¨¡æ€æ•°æ®é›†ï¼Œæ¶µç›– 10 ç§æ¨¡æ€çš„2500ä¸‡å¼ å›¾åƒï¼Œä¸º65ç§ç–¾ç—…æä¾›å¤šç²’åº¦æ³¨é‡Š | [HF](https://huggingface.co/datasets/UCSC-VLAA/MedTrinity-25M) |
|  MediConfusion | [MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models](https://arxiv.org/abs/2409.15477) | ICLR 2025 | å›¾åƒ + æ–‡æœ¬ | åŒ»å­¦VQA | ç”± 176 ä¸ªä»¤äººå›°æƒ‘çš„å¯¹ç»„æˆã€‚æ··æ·†å¯¹æ˜¯ä¸€ç»„ä¸¤å¼ å›¾åƒï¼Œå®ƒä»¬å…±äº«ç›¸åŒçš„é—®é¢˜å’Œç›¸åº”çš„ç­”æ¡ˆé€‰é¡¹ï¼Œä½†å›¾åƒçš„æ­£ç¡®ç­”æ¡ˆä¸åŒã€‚ | [HF](https://huggingface.co/datasets/shahab7899/MediConfusion) |
| GMAI-MMBench       | GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI | NeurIPS 2024          | å¤šæ¨¡æ€ï¼ˆ38ç§ï¼‰            | åŒ»å­¦VQA                         | 26Ké—®ç­”å¯¹                      | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                     |
| PathMMU            | PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Patholog | 2024.03.20      | ç—…ç†å›¾åƒ + æ–‡æœ¬           | é€‰æ‹©é¢˜+æ¨ç†                     | 33,428é—®ç­”, 24,067å›¾åƒ         | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                           |
| OmniMedVQA         | OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM    | CVPR 2024                 | å¤šæ¨¡æ€ï¼ˆ12ç§ï¼‰            | åŒ»å­¦VQA                         | 118,010å›¾åƒ, 127,995é—®ç­”       | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                     |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models | NeurIPS 2024         | åŒ»å­¦å›¾åƒ+é—®ç­”             | å¼€æ”¾ä¸å°é—­é—®ç­”                   | 41Ké—®ç­”å¯¹                      | [GitHub](https://github.com/richard-peng-xia/CARES)                                              |
| MultiMedEval       | MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models  | 2024.02.16                | å›¾æ–‡å¤šæ¨¡æ€                | å¤šä»»åŠ¡è¯„ä¼°                      | 6ä»»åŠ¡, 23æ•°æ®é›†                | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                           |                                   |
| medical-o1-reasoning-SFT       | HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs | ACL 2025          | åŒ»å­¦VQAã€æ¨ç†            | åŒ»å­¦VQA                         | 19.7ké—®ç­”å¯¹                      | [HF](https://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT)                                     |

---

## ğŸ“¬ è”ç³»æˆ‘ä»¬ / Issues & Contact

å¦‚æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿æäº¤ Issue æˆ–é€šè¿‡é‚®ä»¶è”ç³»ï¼š

- ğŸ“§ **å¶èµæŒº (Zanting Ye)**: yzt2861252880@gmail.com  
- ğŸ“§ **éŸ©ç»ª (Xu Han)**: hanxv8826@gmail.com

---

## ğŸ‘¥ åˆä½œè€…ï¼ˆContributorsï¼‰

<table>
  <tr>
    <td align="center">
      <a href="https://github.com/yezanting">
        <img src="https://github.com/yezanting.png" width="80px;" alt="yezanting"/><br />
        <sub><b>å¶èµæŒº(Zanting Ye)</b></sub><br />
        <a href="mailto:yzt2861252880@gmail.com">ğŸ“§</a>
      </a>
    </td>
    <td align="center">
      <a href="https://github.com/lailainan">
        <img src="https://github.com/lailainan.png" width="80px;" alt="lailainan"/><br />
        <sub><b>éŸ©ç»ª(Xu Han)</b></sub><br />
        <a href="mailto:hanxv8826@gmail.com">ğŸ“§</a>
      </a>
    </td>
        <td align="center">
          <a href="https://github.com/Long0121">
          <img src="https://github.com/Long0121.png" width="80px;" alt="lailainan"/><br />
          <sub><b>ç‰›å°é¾™(Xiaolong Niu)</b></sub><br />
          <a href="">ğŸ“§</a>
     </td>
        <td align="center">
          <a href="https://github.com/zianwang1110">
          <img src="https://github.com/zianwang1110.png" width="80px;" alt="lailainan"/><br />
          <sub><b>ç‹æ¢“å®‰(Zian Wang)</b></sub><br />
          <a href="">ğŸ“§</a>  
      </a>
    </td>
        <td align="center">
          <a href="https://github.com/Saint-lsy">
          <img src="https://github.com/Saint-lsy.png" width="80px;" alt="lailainan"/><br />
          <sub><b>åˆ˜åœ£åœ†(Shengyuan Liu)</b></sub><br />
          <a href="liushengyuan@link.cuhk.edu.hk">ğŸ“§</a> 
      </a>
    </td>
  </tr>
</table>

---

## â­ Star è¶‹åŠ¿å›¾ (Star History)

![Star History Chart](https://api.star-history.com/svg?repos=yezanting/Med-VLM-Bench-Summary&type=Date)

---

---

â­ Star æœ¬é¡¹ç›®æ”¯æŒæˆ‘ä»¬æŒç»­æ›´æ–°ï¼Œæ¬¢è¿ PR å’Œå»ºè®®äº¤æµï¼
