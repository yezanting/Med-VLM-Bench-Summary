# ğŸ§  Med-VLM-Bench: A Curated Benchmark Repository for Medical Vision-Language Models

ğŸ“š A comprehensive summary of recent benchmarks for evaluating and training Medical Vision-Language Models (Med-VLMs)

---

## ğŸ‘¨â€ğŸ’» Contributors

- ğŸ§‘â€ğŸ”¬ **Zanting Ye**  
  Southern Medical University  
  ğŸ“§ yzt2861252880@gmail.com

- ğŸ§‘â€ğŸ”¬ **Xu Han**  
  Shanghai Jiao Tong University  
  ğŸ“§ hanxv8826@gmail.com

- ğŸ§‘â€ğŸ”¬ **Xiaolong Niu**  
  Southern Medical University  

- ğŸ‘¨â€ğŸ« **Lijun Lu**  
  Southern Medical University

---

## ğŸ“Š GitHub Stats

![Stars](https://img.shields.io/github/stars/yezanting/Med-VLM-Bench-Summary?style=social)
![Forks](https://img.shields.io/github/forks/yezanting/Med-VLM-Bench-Summary?style=social)
![License](https://img.shields.io/github/license/yezanting/Med-VLM-Bench-Summary)
![Last Commit](https://img.shields.io/github/last-commit/yezanting/Med-VLM-Bench-Summary)

---

## ğŸ” Project Overview

With the continuous advancement of research on Medical Vision-Language Models (Med-VLMs) and their reasoning capabilities, a number of high-quality, publicly available datasets focusing on medical reasoning have been released between March and May 2025. These datasets provide a solid foundation for the development of multimodal medical AI systems.

**Med-VLM-Bench** is a curated, continuously updated repository of the latest and most important datasets for training and evaluating medical LLMs and VLMs. This project focuses on:

- âœ… Reasoning-centric multimodal benchmarks
- ğŸ“… Latest datasets published in Marâ€“May 2025
- ğŸ§  Foundational datasets from 2023â€“2024
- ğŸ”— Direct access to dataset links or HuggingFace/GitHub repositories

ğŸ’¡ Our knowledge is limited to public sources. We welcome community contributions â€” feel free to open an issue to share new datasets, and we will update promptly. 

ğŸ“ŒNote: The annotation time of the dataset is based on the publication time of the corresponding article.

---

## ğŸ“Š Dataset Summary Table

| Dataset Name       | Paper Title                                                                                     | Year / Venue            | Data Modality              | Task Type                      | Size                          | Download Link                                                                                      |
|--------------------|--------------------------------------------------------------------------------------------------|--------------------------|----------------------------|--------------------------------|-------------------------------|-----------------------------------------------------------------------------------------------------|
| ReasonMed| [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513) | 2025.06.11 | Text (Multi-agent CoT, Summary, QA) | Medical Reasoning, QA, CoT Fine-tuning | 370K high-quality samples distilled from 1.75M CoT paths, based on 195K questions from 4 benchmarks | [HF](https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed) |
| Lingshu (Train) | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | Text + Image (Multimodal Instruction, VQA, Report) | Multimodal Medical QA, Reasoning, Consultation, Report Generation | ~9.3M training samples from 60+ datasets | [Project Page](https://alibaba-damo-academy.github.io/lingshu/) |
| MedEvalKit (Linshu Test) | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | Text + Image (Multimodal Benchmarks) | Benchmarking: VQA, Report Generation, Medical Text QA | 152,066 evaluation samples from 16 benchmarks | [Github](https://github.com/alibaba-damo-academy/MedEvalKit) |
| MIRIAD | [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091) | 2025.06.09 | Text (Instruction-Response) | Medical QA, Retrieval-Augmented Generation (RAG), Hallucination Detection | 5.8M / 4.4M QA pairs | [HF](https://huggingface.co/datasets/miriad/miriad-5.8M) |
| SurgVLM-DB         |SurgVLM-DB: A Large-scale Multimodal Surgical Database Comprising Over 1.81 Million Frames with 7.79 Million Conversations                                                        | 2025.06                 | Video + Text               | Multimodal QA                  | 1.81M frames, 7.79M QAs       | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                   |
| MedXpertQA | MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding | ICML2025 | Text + Image (Multimodal MCQs) | Expert-level Medical QA, Clinical Reasoning, Multimodal Understanding | 4,460 questions (2,455 text / 2,005 image) | [HF](https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA) |
| MedCaseReasoning   | MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports                         | 2025.05.20              | Text                       | Diagnostic Reasoning           | 14,489 QA cases               | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                   |
| vlm-project-with-images-with-bbox-images-with-tree-of-thoughts | - | 2025.06                 | MRI Image + BBox + Multilingual QA (Including 7 languages: vi, en, fr, de, zh, ko, ja) | VQA, Lesion Detection, Clinical Reasoning (Tree-of-Thought) | 12.3k samples | [HF](https://huggingface.co/datasets/tungvu3196/vlm-project-with-images-with-bbox-images-with-tree-of-thoughts) | N/A |
| DrVD-Bench         | DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?                | 2025.05.30              | Medical Images + Text      | VQA, Reasoning, Report Gen.    | 7,789 imageâ€“QA pairs          | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine                                     | 2025.05                 | Text                       | Instruction Tuning             | 5M instances, 19K instructions| [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                            |
| MedS-Bench         | -                                                                                                | 2025.05                 | Text                       | Clinical Task Benchmark        | 11 task types                 | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                          |
| MM-Skin            | MM-Skin:Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks                    | 2025.05.09              | Image + Text               | Open-ended VQA (no reasoning)  | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                        |
| AlphaMed19K        | Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL                                                     | 2025.05.23              | Text                       | QA Reasoning                   | 19K QAs                       | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                           |
| PMC-VQA            | PMC-VQA: Visual Instruction Tuning for Medical VQA                                                       | 2024.08.08              | Image + Text               | VQA                            | 226,946 QA pairs              | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                            |
| VQARad             | - | 2023.08.07              | Radiography                | VQA                            | 315 images, 3,515 QAs         | [OSF](https://osf.io/89kps/)                                                                         |
| GMAI-MMBench       | GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI                     | NeurIPS 2024            | Multi-modal (38 types)     | VQA                            | 26K QA pairs                  | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                       |
| PathMMU            | PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology       | 2024.03.20              | Pathology Image + Text     | Multi-choice, Reasoning        | 33,428 QAs, 24,067 images     | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                             |
| OmniMedVQA         | OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM                          | CVPR 2024               | Multi-modal (12 types)     | VQA                            | 118,010 images, 127,995 QA    | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                       |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models                 | NeurIPS 2024            | Medical Images + QA        | Open/Closed QA                 | 41K QA pairs                  | [GitHub](https://github.com/richard-peng-xia/CARES)                                                |
| MultiMedEval       | MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models                        | 2024.02.16              | Image + Text               | Multi-task Evaluation          | 6 tasks, 23 datasets          | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                             |

---

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆæœ¬

### ğŸ” é¡¹ç›®ç®€ä»‹

éšç€åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆMed-VLMï¼‰åŠå…¶æ¨ç†èƒ½åŠ›ç ”ç©¶çš„æŒç»­æ¨è¿›ï¼Œå°¤å…¶åœ¨ 2025 å¹´ 3 æœˆè‡³ 5 æœˆæœŸé—´ï¼Œé™†ç»­å‘å¸ƒäº†ä¼—å¤šé«˜è´¨é‡ã€èšç„¦äºåŒ»å­¦æ¨ç†èƒ½åŠ›çš„æ–°å‹å…¬å¼€æ•°æ®é›†ï¼Œä¸ºå¤šæ¨¡æ€åŒ»ç–—äººå·¥æ™ºèƒ½çš„å‘å±•æä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½æ±‡æ€»è¿™ä¸€äº›æ•°æ®ï¼ŒæœŸå¾…èƒ½ä¸ºè¯¥ç¤¾åŒºæä¾›æ›´ä¾¿æ·çš„æ•°æ®è®¿é—®æ–¹å¼ã€‚æˆ‘ä»¬å‘å¸ƒäº†Med-VLM-Benchï¼š

**Med-VLM-Bench** è‡´åŠ›äºæ±‡æ€»å¹¶æ•´ç†è¿™äº›æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°çš„å…³é”®èµ„æºï¼š

- âœ… èšç„¦ 2025 å¹´ 3â€“6 æœˆå‘å¸ƒçš„æ–°æ•°æ®é›†  
- ğŸ§  é‡ç‚¹å¼ºè°ƒæ¨ç†èƒ½åŠ›ã€å¤šæ¨¡æ€ç†è§£å’Œé—®ç­”èƒ½åŠ›çš„æ•°æ®é›† 
- ğŸ§ª åŒæ—¶è¦†ç›– 2023â€“2024 å¹´çš„ç»å…¸Med LLM/VLM benchmark datasets
- ğŸ”— æä¾›ç›´æ¥å¯ç”¨çš„ä¸‹è½½é“¾æ¥å’Œå¼€æºåœ°å€

ğŸ’¡ æˆ‘ä»¬çš„çŸ¥è¯†æ¥æºæœ‰é™ï¼Œæ¬¢è¿å¤§å®¶é€šè¿‡ Issue æˆ– PR æ¨èæ›´å¤šæ•°æ®é›†ï¼Œæˆ‘ä»¬ä¼šç¬¬ä¸€æ—¶é—´æ›´æ–°ï¼

ğŸ“ŒNote: æ­¤å¤–æˆ‘ä»¬çš„æ•°æ®é›†æ ‡æ³¨æ—¶é—´ä»¥ç›¸åº”æ–‡ç« å‘è¡¨æ—¶é—´ä¸ºå‡†

---

### ğŸ“Š æ•°æ®é›†æ±‡æ€»è¡¨

| æ•°æ®é›†åç§°         | è®ºæ–‡æ ‡é¢˜                                                                 | å¹´ä»½ / ä¼šè®®              | æ•°æ®æ¨¡æ€                  | ä»»åŠ¡ç±»å‹                        | æ•°æ®è§„æ¨¡                       | ä¸‹è½½é“¾æ¥                                                                                         |
|--------------------|--------------------------------------------------------------------------|---------------------------|---------------------------|----------------------------------|-------------------------------|--------------------------------------------------------------------------------------------------|
| ReasonMed | [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513) | 2025.06.11 | æ–‡æœ¬ï¼ˆå¤šä»£ç†æ¨ç†ã€å¤šæ­¥æ€»ç»“ã€åŒ»å­¦é—®ç­”ï¼‰ | åŒ»å­¦æ¨ç†ã€é—®ç­”ã€é“¾å¼æ€ç»´å¾®è°ƒ | ä» 175 ä¸‡æ¡ CoT è·¯å¾„ä¸­ç²¾ç‚¼å‡ºçš„ 37 ä¸‡é«˜è´¨é‡æ ·æœ¬ï¼Œè¦†ç›–æ¥è‡ª 4 ä¸ªåŸºå‡†çš„ 19.5 ä¸‡é—®é¢˜ | [HF](https://huggingface.co/datasets/lingshu-medical-mllm/ReasonMed) |
| Lingshuï¼ˆTrainï¼‰ | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | æ–‡æœ¬ + å›¾åƒï¼ˆå¤šæ¨¡æ€æŒ‡ä»¤ã€VQAã€æŠ¥å‘Šï¼‰ | å¤šæ¨¡æ€åŒ»å­¦é—®ç­”ã€æ¨ç†ã€é—®è¯Šã€æŠ¥å‘Šç”Ÿæˆ | çº¦ 930 ä¸‡è®­ç»ƒæ ·æœ¬ï¼Œæ¥è‡ª 60+ æ•°æ®é›† | [Project Page](https://alibaba-damo-academy.github.io/lingshu/) |
| MedEvalKitï¼ˆLinshu testï¼‰ | [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/pdf/2506.07044) | 2025.06.08 | æ–‡æœ¬ + å›¾åƒï¼ˆå¤šæ¨¡æ€è¯„æµ‹åŸºå‡†ï¼‰ | åŸºå‡†è¯„æµ‹ï¼šVQAã€æŠ¥å‘Šç”Ÿæˆã€åŒ»å­¦æ–‡æœ¬é—®ç­” | å…± 152,066 ä¸ªè¯„ä¼°æ ·æœ¬ï¼Œæ¥è‡ª 16 ä¸ªåŸºå‡†æ•°æ®é›† | [Github](https://github.com/alibaba-damo-academy/MedEvalKit) |
| MIRIAD | [MIRIAD: Augmenting LLMs with millions of medical query-response pairs](https://arxiv.org/abs/2506.06091) | 2025.06.09 | æ–‡æœ¬ï¼ˆæŒ‡ä»¤-å›ç­”å¯¹ï¼‰ | åŒ»å­¦é—®ç­”ã€RAG æ£€ç´¢å¢å¼ºã€å¹»è§‰æ£€æµ‹ |  582 ä¸‡ / 448 ä¸‡ | [HF](https://huggingface.co/datasets/miriad/miriad-5.8M) |
| SurgVLM-DB         |SurgVLM-DB: A Large-Scale Multimodal Surgical Database                               | 2025.06                   | è§†é¢‘ + æ–‡æœ¬               | å¤šæ¨¡æ€é—®ç­”                      | 1.81Må¸§, 7.79Må¯¹è¯             | [GitHub](https://github.com/jinlab-imvr/SurgVLM)                                                 |
| MedXpertQA | MedXpertQA: Benchmarking Expert-Level Medical Reasoning and Understanding | ICML2025 | æ–‡æœ¬ + å›¾åƒï¼ˆå¤šæ¨¡æ€é€‰æ‹©é¢˜ï¼‰ | ä¸“å®¶çº§åŒ»å­¦é—®ç­”ã€ä¸´åºŠæ¨ç†ã€å¤šæ¨¡æ€ç†è§£ | å…± 4,460 é¢˜ï¼ˆæ–‡æœ¬ 2,455 / å›¾åƒ 2,005ï¼‰ | [HF](https://huggingface.co/datasets/TsinghuaC3I/MedXpertQA) |
| MedCaseReasoning   |MedCaseReasoning: Evaluating and Learning Diagnostic Reasoning from Clinical Case Reports  | 2025.05.20                | æ–‡æœ¬                      | è¯Šæ–­æ¨ç†                        | 14,489é—®ç­”å¯¹                   | [GitHub](https://github.com/kevinwu23/Stanford-MedCaseReasoning)                                 |
| vlm-project-with-images-with-bbox-images-with-tree-of-thoughts | - | 2025.06 | MRI å›¾åƒ + BBox + å¤šè¯­ç§é—®ç­”ï¼ˆvi, en, fr, de, zh, ko, jaï¼‰ | åŒ»å­¦VQAã€ç—…ç¶æ£€æµ‹ã€ä¸´åºŠæ¨ç†ï¼ˆTree-of-Thoughtï¼‰ | 12,325 æ¡æ ·æœ¬ | [HF](https://huggingface.co/datasets/tungvu3196/vlm-project-with-images-with-bbox-images-with-tree-of-thoughts) | æœªæ³¨æ˜ |
| DrVD-Bench         | DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis? | 2025.05.30          | åŒ»å­¦å›¾åƒ + æ–‡æœ¬           | åŒ»å­¦VQAã€æ¨ç†ã€æŠ¥å‘Šç”Ÿæˆ         | 7,789å›¾æ–‡QAå¯¹                  | [GitHub](https://github.com/Jerry-Boss/DrVD-Bench), [HF](https://huggingface.co/datasets/jerry1565/DrVD-Bench) |
| MedS-Ins           | Towards Evaluating and Building Versatile LLMs for Medicine              | 2025.05                   | æ–‡æœ¬                      | æŒ‡ä»¤å¾®è°ƒ                        | 5Mæ ·æœ¬, 19KæŒ‡ä»¤                | [HF](https://huggingface.co/datasets/Henrychur/MedS-Ins)                                         |
| MedS-Bench         | -                                                                         | 2025.05                   | æ–‡æœ¬                      | ä¸´åºŠä»»åŠ¡è¯„ä¼°                    | 11å¤§ç±»ä»»åŠ¡                     | [HF](https://huggingface.co/datasets/Henrychur/MedS-Bench)                                       |
| MM-Skin            | MM-Skin:Enhancing Dermatology VLM with an Image-Text Dataset Derived from Textbooks | 2025.05.09          | å›¾åƒ + æ–‡æœ¬               | å¼€æ”¾å¼VQAï¼ˆæ— æ¨ç†ï¼‰             | -                             | [GitHub](https://github.com/ZwQ803/MM-Skin)                                                      |
| AlphaMed19K        | Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL                              | 2025.05.23                | æ–‡æœ¬                      | æ¨ç†é—®ç­”                        | 19Ké—®ç­”å¯¹                      | [HF](https://huggingface.co/datasets/che111/AlphaMed19K)                                         |
| PMC-VQA            | PMC-VQA: Visual Instruction Tuning for Medical VQA                                 | 2024.08.08                | å›¾åƒ + æ–‡æœ¬               | åŒ»å­¦VQA                         | 226,946é—®ç­”å¯¹                  | [HF](https://huggingface.co/datasets/RadGenome/PMC-VQA)                                          |
| VQARad             |-| 2023.08.07                | æ”¾å°„å›¾åƒ                  | VQA                             | 315å›¾åƒ, 3515é—®ç­”              | [OSF](https://osf.io/89kps/)                                                                       |
| GMAI-MMBench       | GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI | NeurIPS 2024          | å¤šæ¨¡æ€ï¼ˆ38ç§ï¼‰            | åŒ»å­¦VQA                         | 26Ké—®ç­”å¯¹                      | [HF](https://huggingface.co/datasets/OpenGVLab/GMAI-MMBench)                                     |
| PathMMU            | PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Patholog | 2024.03.20      | ç—…ç†å›¾åƒ + æ–‡æœ¬           | é€‰æ‹©é¢˜+æ¨ç†                     | 33,428é—®ç­”, 24,067å›¾åƒ         | [HF](https://huggingface.co/datasets/jamessyx/PathMMU)                                           |
| OmniMedVQA         | OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM    | CVPR 2024                 | å¤šæ¨¡æ€ï¼ˆ12ç§ï¼‰            | åŒ»å­¦VQA                         | 118,010å›¾åƒ, 127,995é—®ç­”       | [OpenXLab](https://openxlab.org.cn/datasets/GMAI/OmniMedVQA)                                     |
| CARES              | A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models | NeurIPS 2024         | åŒ»å­¦å›¾åƒ+é—®ç­”             | å¼€æ”¾ä¸å°é—­é—®ç­”                   | 41Ké—®ç­”å¯¹                      | [GitHub](https://github.com/richard-peng-xia/CARES)                                              |
| MultiMedEval       | MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models  | 2024.02.16                | å›¾æ–‡å¤šæ¨¡æ€                | å¤šä»»åŠ¡è¯„ä¼°                      | 6ä»»åŠ¡, 23æ•°æ®é›†                | [GitHub](https://github.com/corentin-ryr/MultiMedEval)                                           |

---

## ğŸ“¬ è”ç³»æˆ‘ä»¬ / Issues & Contact

å¦‚æœ‰ä»»ä½•é—®é¢˜æ¬¢è¿æäº¤ Issue æˆ–é€šè¿‡é‚®ä»¶è”ç³»ï¼š

- ğŸ“§ **å¶èµæŒº (Zanting Ye)**: yzt2861252880@gmail.com  
- ğŸ“§ **éŸ©ç»ª (Xu Han)**: hanxv8826@gmail.com

---

## ğŸ‘¥ åˆä½œè€…ï¼ˆContributorsï¼‰

<table>
  <tr>
    <td align="center">
      <a href="https://github.com/yezanting">
        <img src="https://github.com/yezanting.png" width="80px;" alt="yezanting"/><br />
        <sub><b>å¶èµæŒº</b></sub><br />
        <a href="mailto:yzt2861252880@gmail.com">ğŸ“§</a>
      </a>
    </td>
    <td align="center">
      <a href="https://github.com/lailainan">
        <img src="https://github.com/lailainan.png" width="80px;" alt="lailainan"/><br />
        <sub><b>éŸ©ç»ª</b></sub><br />
        <a href="mailto:hanxv8826@gmail.com">ğŸ“§</a>
      </a>
    </td>
  </tr>
</table>

---

## â­ Star è¶‹åŠ¿å›¾ (Star History)

![Star History Chart](https://api.star-history.com/svg?repos=yezanting/Med-VLM-Bench-Summary&type=Date)

---

---

â­ Star æœ¬é¡¹ç›®æ”¯æŒæˆ‘ä»¬æŒç»­æ›´æ–°ï¼Œæ¬¢è¿ PR å’Œå»ºè®®äº¤æµï¼
